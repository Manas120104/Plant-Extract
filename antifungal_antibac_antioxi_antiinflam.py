# -*- coding: utf-8 -*-
"""Antifungal_antibac_antioxi_antiinflam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Tm7unNILQ9UBt7BiPjTNa9gTVu9Hr8H
"""

# Importing all the necessary libraries
import numpy as np
import pandas as pd
# Set Pandas display options
#pd.set_option('display.max_rows', None) //Makes the entire dataset visible in case of large dataset
#pd.set_option('display.max_columns', None)
# For Visualization
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.compose import ColumnTransformer
import scipy.stats as stats

import warnings
def warns(*args,**kwargs): pass
warnings.warn=warns

# Reading the dataset from a csv file
df = pd.read_csv('/content/copper_nanoparticle_dataset (1).csv')
print('Data Shape: ', df.shape)
df.sample(10)

# Information about the dataset
df.info()

# Checking for null values in the dataset
df.isnull().sum()

# Summary of statistics
df.describe()

# Checking for duplicated values
df.duplicated().sum()

# Number of Records Against Each Labels: Antifungal_Activity	Antibacterial_Activity	Antioxidant_Activity	Anti_Inflammatory_Activity
print("Number of Records Against Each Labels for Antifungal_Activity, Antibacterial_Activity,	Antioxidant_Activity,	Anti_Inflammatory_Activity")
print("-"*35)
print(df['Antifungal_Activity'].value_counts() )
print(df['Antibacterial_Activity'].value_counts() )
print(df['Antioxidant_Activity'].value_counts() )
print(df['Anti_Inflammatory_Activity'].value_counts() )

# Column names
print(df.columns)

# dropping unnecessary column
df = df.drop(['Sample_ID', 'Plant_Material'], axis=1)
df

# Input features
selected_features = ['UV_Visible', 'SEM', 'TEM', 'EDAX','XRD', 'Particle_Size']
selected_features

numeric_columns = df.select_dtypes(include=[np.number])
# Rank the data
ranked_data = numeric_columns.rank()
# Calculate Spearman's correlation
corr_spearman = ranked_data.corr(method='spearman')
# Calculate Pearson correlation
corr_pearson = numeric_columns.corr(method='pearson')
# Calculate Kendall's Tau correlation
corr_kendall = ranked_data.corr(method='kendall')

ranked_data

corr_pearson

corr_spearman

corr_kendall

# Heatmap for pearson correlation matrix
plt.figure(figsize=(12,8))
plt.title("Features Correlation using Pearson Correlation Matrix", fontsize=18)
sns.heatmap(corr_pearson, annot=True)

# Heatmap for Spearman's correlation matrix
plt.figure(figsize=(12,8))
plt.title("Features Correlation using Spearman's Correlation Matrix", fontsize=18)
sns.heatmap(corr_spearman, annot=True)

# Heatmap for Kendall's Tau correlation matrix
plt.figure(figsize=(12,8))
plt.title("Features Correlation using Kendall's Tau Correlation Matrix", fontsize=18)
sns.heatmap(corr_kendall, annot=True)

# Correlation of each feature with the target column Antifungal_Activity

target_correlation = corr_pearson['Antifungal_Activity'].sort_values(ascending=False)

# Drop the target itself from the correlation results
target_correlation = target_correlation.drop('Antifungal_Activity')

print("Correlation of features with target Antifungal_Activity:\n", target_correlation)

# Plot the correlations
plt.figure(figsize=(25, 15))
sns.barplot(x=target_correlation.index, y=target_correlation.values)
plt.title('Correlation of Features with Target Antifungal_Activity')
plt.xlabel('Features')
plt.ylabel('Correlation Coefficient')
plt.show()

# Correlation of each feature with the target column Antibacterial_Activity

target_correlation = corr_pearson['Antibacterial_Activity'].sort_values(ascending=False)

# Drop the target itself from the correlation results
target_correlation = target_correlation.drop('Antibacterial_Activity')

print("Correlation of features with target Antibacterial_Activity:\n", target_correlation)

# Plot the correlations
plt.figure(figsize=(25, 15))
sns.barplot(x=target_correlation.index, y=target_correlation.values)
plt.title('Correlation of Features with Target Antibacterial_Activity')
plt.xlabel('Features')
plt.ylabel('Correlation Coefficient')
plt.show()

# Correlation of each feature with the target column Antioxidant_Activity

target_correlation = corr_pearson['Antioxidant_Activity'].sort_values(ascending=False)

# Drop the target itself from the correlation results
target_correlation = target_correlation.drop('Antioxidant_Activity')

print("Correlation of features with target Antioxidant_Activity:\n", target_correlation)

# Plot the correlations
plt.figure(figsize=(25, 15))
sns.barplot(x=target_correlation.index, y=target_correlation.values)
plt.title('Correlation of Features with Target Antioxidant_Activity')
plt.xlabel('Features')
plt.ylabel('Correlation Coefficient')
plt.show()

# Correlation of each feature with the target column Anti_Inflammatory_Activity

target_correlation = corr_pearson['Anti_Inflammatory_Activity'].sort_values(ascending=False)

# Drop the target itself from the correlation results
target_correlation = target_correlation.drop('Anti_Inflammatory_Activity')

print("Correlation of features with target Anti_Inflammatory_Activity:\n", target_correlation)

# Plot the correlations
plt.figure(figsize=(25, 15))
sns.barplot(x=target_correlation.index, y=target_correlation.values)
plt.title('Correlation of Features with Target Anti_Inflammatory_Activity')
plt.xlabel('Features')
plt.ylabel('Correlation Coefficient')
plt.show()

# Histogram for univariate analysis
for x in selected_features:
    ax = sns.set_style('whitegrid')
    plt.subplots(figsize=(9,8))
    plt.hist(df[x])
    plt.xticks(rotation=90, fontsize=14)
    plt.yticks(rotation=0, fontsize=14)
    plt.title("Analysing the " + str(x) + " feature" ,fontsize = 24)
    plt.xlabel(str(x), fontsize = 18)
    plt.ylabel("Frequency of data points", fontsize = 18)

# DISTPLOT AND PROBABILITY PLOT FOR UNIVARIATE ANALYSIS
for x in selected_features:
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the distribution (distplot) on the first subplot
    sns.set_style('whitegrid')
    sns.distplot(df[x], ax=axes[0])
    axes[0].set_title("Distribution of " + x, fontsize=14)

    # Plot the Probability plot on the second subplot
    stats.probplot(df[x], dist="norm", plot=axes[1])
    axes[1].set_title("Probability Plot of " + x, fontsize=14)

    plt.show()

# BOXPLOT FOR STATISTIC VISUALISATION OF EACH FEATURE AND OUTLIER DETECTION
for x in selected_features:
    plt.figure(figsize=(8, 6))

    sns.set_style('whitegrid')
    sns.boxplot(x=df[x])

    plt.title(x, fontsize=20)
    plt.xlabel("Values of " + x, fontsize=18)

    plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Pair plot for 'Antifungal_Activity'
sns.pairplot(df, hue='Antifungal_Activity')
plt.suptitle('Pairplot of Features with Antifungal Activity', fontsize=30)  # Increased font size for title
plt.subplots_adjust(top=0.92)  # Adjusting space for the title

# Increase the size of the x and y labels
plt.xlabel('Features', fontsize=20)  # Increase x label size
plt.ylabel('Features', fontsize=20)  # Increase y label size

plt.show()

# Pair plot for 'Antibacterial_Activity'
sns.pairplot(df, hue='Antibacterial_Activity')
plt.suptitle('Pairplot of Features with Antibacterial Activity', fontsize=30)  # Increased font size for title
plt.subplots_adjust(top=0.92)  # Adjusting space for the title

# Increase the size of the x and y labels
plt.xlabel('Features', fontsize=20)  # Increase x label size
plt.ylabel('Features', fontsize=20)  # Increase y label size

plt.show()

# Pair plot for 'Antioxidant_Activity'
sns.pairplot(df, hue='Antioxidant_Activity')
plt.suptitle('Pairplot of Features with Antioxidant Activity', fontsize=30)  # Increased font size for title
plt.subplots_adjust(top=0.92)  # Adjusting space for the title

# Increase the size of the x and y labels
plt.xlabel('Features', fontsize=20)  # Increase x label size
plt.ylabel('Features', fontsize=20)  # Increase y label size

plt.show()

# Pair plot for 'Anti_Inflammatory_Activity'
sns.pairplot(df, hue='Anti_Inflammatory_Activity')
plt.suptitle('Pairplot of Features with Anti-Inflammatory Activity', fontsize=30)  # Increased font size for title
plt.subplots_adjust(top=0.92)  # Adjusting space for the title

# Increase the size of the x and y labels
plt.xlabel('Features', fontsize=20)  # Increase x label size
plt.ylabel('Features', fontsize=20)  # Increase y label size

plt.show()

# DATA TRANSFORMATION

# BEFORE TRANSFORMATION

for x in selected_features:
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the distribution (distplot) on the first subplot
    sns.set_style('whitegrid')
    sns.distplot(df[x], ax=axes[0])
    axes[0].set_title("Distribution of " + x, fontsize=14)

    # Plot the Probability plot on the second subplot
    stats.probplot(df[x], dist="norm", plot=axes[1])
    axes[1].set_title("Probability Plot of " + x, fontsize=14)

    plt.show()

# APPLYING YEO-JOHNSON POWER TRANSFORMATION

from sklearn.preprocessing import PowerTransformer

numeric_columns = df.drop(['Antifungal_Activity', 'Antibacterial_Activity','Antioxidant_Activity','Anti_Inflammatory_Activity'], axis=1)

# Create a PowerTransformer object using the Yeo-Johnson transformation
pt = PowerTransformer(method='yeo-johnson')

# Fit and transform the numeric columns using the Yeo-Johnson transformation
transformed_data = pt.fit_transform(numeric_columns)

# Replace the original numeric columns with the transformed data
df[numeric_columns.columns] = transformed_data

# AFTER TRANSFORMATION

for x in selected_features:
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the distribution (distplot) on the first subplot
    sns.set_style('whitegrid')
    sns.distplot(df[x], ax=axes[0])
    axes[0].set_title("Distribution of " + x, fontsize=14)

    # Plot the Probability plot on the second subplot
    stats.probplot(df[x], dist="norm", plot=axes[1])
    axes[1].set_title("Probability Plot of " + x, fontsize=14)

    plt.show()

df

df.describe()

"""# **MODEL-1: FEEDFORWARD NEURAL NETWORK FOR EVERY TARGET LABEL**

## **1.A FEEDFORWARD NEURAL NETWORK MODEL FOR PREDICTING ANTIFUNGAL_ACTIVITY**

## **1.B FEEDFORWARD NEURAL NETWORK MODEL FOR PREDICTING ANTIBACTERIAL_ACTIVITY**

## **1.C FEEDFORWARD NEURAL NETWORK MODEL FOR PREDICTING ANTIOXIDANT_ACTIVITY**

## **1.D FEEDFORWARD NEURAL NETWORK MODEL FOR PREDICTING ANTI_INFLAMMATORY_ACTIVITY**
"""

yf = df['Antifungal_Activity']  # Targets: Antifungal_Activity,	Antibacterial_Activity,	Antioxidant_Activity,	Anti_Inflammatory_Activity
yb = df['Antibacterial_Activity']
yo = df['Antioxidant_Activity']
yi = df['Anti_Inflammatory_Activity']
X = df[selected_features]  # Input Features
X

yf

yb

yo

yi

# DATA SPLITTING
from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split

# Train-test split
X_train, X_test, y_trainb, y_testb = train_test_split(X, yb, test_size=0.3, random_state=0)
_, _, y_trainf, y_testf = train_test_split(X, yf, test_size=0.3, random_state=0)
_, _, y_traino, y_testo = train_test_split(X, yo, test_size=0.3, random_state=0)
_, _, y_traini, y_testi = train_test_split(X, yi, test_size=0.3, random_state=0)

# Import necessary libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dropout

# Install keras-tuner for hyperparameter tuning
!pip install -U keras-tuner

import kerastuner as kt

# function for hyperparameter tuning

def BuildModel(hp):
    model = Sequential()
    counter = 0
    for i in range(hp.Int('num_layers', min_value=1, max_value=10)):
        if counter == 0:
            # First layer: Include input_dim for the first layer
            model.add(Dense(hp.Int('units'+str(i), min_value=8, max_value=128, step=8),
                            activation=None,  # Set activation to None for now
                            input_dim=X_train.shape[1]))

            # Batch Normalization as a tunable option
            if hp.Boolean('batch_norm'+str(i)):
                model.add(BatchNormalization())

            # Now apply the activation function
            model.add(Activation(hp.Choice('activation'+str(i), values=['relu', 'tanh', 'sigmoid'])))

            # Dropout
            model.add(Dropout(hp.Choice('dropout'+str(i), values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])))
        else:
            # Subsequent layers
            model.add(Dense(hp.Int('units'+str(i), min_value=8, max_value=128, step=8), activation=None))

            # Batch Normalization as a tunable option
            if hp.Boolean('batch_norm'+str(i)):
                model.add(BatchNormalization())

            # Apply the activation function
            model.add(Activation(hp.Choice('activation'+str(i), values=['relu', 'tanh', 'sigmoid'])))

            # Dropout
            model.add(Dropout(hp.Choice('dropout'+str(i), values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])))

        counter += 1

    # Output layer
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model with a tunable optimizer
    model.compile(optimizer=hp.Choice('optimizer', values=['rmsprop', 'adam', 'sgd', 'nadam', 'adadelta']),
                  loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Early stopping mechanism to monitor model performance

from tensorflow.keras.callbacks import EarlyStopping
# Define the early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',       # Metric to monitor
    patience=5,               # Number of epochs to wait for improvement
    restore_best_weights=True # Restore the model weights from the epoch with the best value of the monitored metric
)

tunerf = kt.RandomSearch(BuildModel,objective='val_accuracy',max_trials=50,directory='mydirf',project_name='finalf')

tunerb = kt.RandomSearch(BuildModel,objective='val_accuracy',max_trials=50,directory='mydirb1',project_name='finalb1')

tunero = kt.RandomSearch(BuildModel,objective='val_accuracy',max_trials=50,directory='mydiro2',project_name='finalo2')

tuneri = kt.RandomSearch(BuildModel,objective='val_accuracy',max_trials=50,directory='mydiri',project_name='finali')

# Hyperparameter tuning for Antifungal_Activity
tunerf.search(X_train,y_trainf,epochs=100,validation_data=(X_test,y_testf),callbacks=[early_stopping])

# Hyperparameter tuning for	Antibacterial_Activity
tunerb.search(X_train,y_trainb,epochs=100,validation_data=(X_test,y_testb),callbacks=[early_stopping])

# Hyperparameter tuning for Antioxidant_Activity
tunero.search(X_train,y_traino,epochs=100,validation_data=(X_test,y_testo),callbacks=[early_stopping])

# Hyperparameter tuning for Anti_Inflammatory_Activity
tuneri.search(X_train,y_traini,epochs=100,validation_data=(X_test,y_testi),callbacks=[early_stopping])

# Retrieve the Best Hyperparameters for Antifungal_Activity
best_hyperparametersf = tunerf.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters for Antifungal_Activity:")
print(best_hyperparametersf.values)

# Retrieve all the best models based on the test accuracy
best_models = tunerb.get_best_models(num_models=10)  # Retrieve top N models (adjust N as per your need)
best_hyperparameters = tunerb.get_best_hyperparameters(num_trials=10)  # Retrieve their hyperparameters

# Initialize variables to track the best model
best_accuracy = 0
best_model = None
best_hparams = None
most_layers = 0
most_units = 0

# Loop through all the best models
for i in range(len(best_models)):
    model = best_models[i]
    hparams = best_hyperparameters[i]

    # Evaluate the model's accuracy
    test_loss, test_accuracy = model.evaluate(X_test, y_testb, verbose=0)

    # Calculate the total number of layers and neurons/units
    total_layers = hparams.get('num_layers')
    total_units = sum([hparams.get(f'units{j}') for j in range(total_layers)])

    # If the accuracy is better, update the best model
    if test_accuracy > best_accuracy:
        best_accuracy = test_accuracy
        best_modelb = model
        best_hyperparametersb = hparams
        most_layers = total_layers
        most_units = total_units
    # If the accuracy is tied, choose the one with more layers and units
    elif test_accuracy == best_accuracy:
        if total_layers > most_layers or (total_layers == most_layers and total_units > most_units):
            best_modelb = model
            best_hyperparametersb = hparams
            most_layers = total_layers
            most_units = total_units

# Print the best hyperparameters
print("Best Hyperparameters:")
print(best_hyperparametersb.values)

# Evaluate the selected best model on the test set
test_loss, test_accuracy = best_modelb.evaluate(X_test, y_testb)
print(f"Best Model Test Loss: {test_loss}")
print(f"Best Model Test Accuracy: {test_accuracy}")

# Save the best model
best_modelb.save('best_modelb.h5')

# Get the Best Model for Antifungal_Activity
best_modelf = tunerf.get_best_models(num_models=1)[0]

# Use the Best Model
# evaluate the model on the test set
test_loss, test_accuracy = best_modelf.evaluate(X_test, y_testf)
print(f"Best Model Test Loss: {test_loss}")
print(f"Best Model Test Accuracy: {test_accuracy}")

best_modelf.summary()

!apt-get install graphviz -y
!pip install pydot
!pip install graphviz

from tensorflow.keras.utils import plot_model

# Assuming you have already defined and compiled your model
plot_model(best_modelf, to_file='modelf.png', show_shapes=True, show_layer_names=True)

# To display the image in Colab:
from IPython.display import Image
Image('modelf.png')

from keras.metrics import Precision, Recall
import tensorflow as tf

# Compile the model without using custom f1 score
best_modelf.compile(
    optimizer=best_hyperparametersf.get('optimizer'),
    loss='binary_crossentropy',
    metrics=['accuracy', Precision(), Recall()]
)

from sklearn.metrics import f1_score
import numpy as np
import tensorflow as tf

# Custom callback to calculate and print validation F1 Score after each epoch
class F1ScoreCallback(tf.keras.callbacks.Callback):
    def __init__(self, validation_data):
        super(F1ScoreCallback, self).__init__()
        self.validation_data = validation_data
        self.val_f1s = []

    def on_epoch_end(self, epoch, logs=None):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        val_f1 = f1_score(val_targ, val_predict, average='macro')
        self.val_f1s.append(val_f1)
        print(f" — val_f1: {val_f1}")

# Initialize the callback
f1_callback = F1ScoreCallback(validation_data=(X_test, y_testf))

# Train the model and collect the history
history = best_modelf.fit(
    X_train, y_trainf,
    epochs=50,
    validation_data=(X_test, y_testf),
    callbacks=[f1_callback]
)

# Extract the training history (for precision, recall, etc.)
history_dict = history.history

# Print history keys to check the metrics available
print(history_dict.keys())

# Extract metrics from the training history
precision = history_dict.get('precision_4', [])  # Adjust based on printed keys
val_precision = history_dict.get('val_precision_4', [])
recall = history_dict.get('recall_4', [])
val_recall = history_dict.get('val_recall_4', [])
f1 = []  # Since we compute F1 manually in the callback
val_f1 = f1_callback.val_f1s

# Plot metrics
epochs = range(1, len(precision) + 1)

plt.figure(figsize=(15, 5))

# Precision plot
plt.subplot(1, 3, 1)
plt.plot(epochs, precision, label='Training Precision')
plt.plot(epochs, val_precision, label='Validation Precision')
plt.title('Training and Validation Precision for Antifungal_Activity')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall plot
plt.subplot(1, 3, 2)
plt.plot(epochs, recall, label='Training Recall')
plt.plot(epochs, val_recall, label='Validation Recall')
plt.title('Training and Validation Recall for Antifungal_Activity')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

# F1 Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs, val_f1, label='Validation F1 Score')
plt.title('Validation F1 Score for Antifungal_Activity')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()

plt.tight_layout()
plt.show()

# Extract accuracy and loss values
acc = history_dict.get('accuracy', [])
val_acc = history_dict.get('val_accuracy', [])
loss = history_dict.get('loss', [])
val_loss = history_dict.get('val_loss', [])

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy for Antifungal_Activity')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss for Antifungal_Activity')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modelf_train_probs = best_modelf.predict(X_train)

# Convert probabilities to binary predictions
threshold = 0.5
best_modelf_train_preds = (best_modelf_train_probs > threshold).astype(int)

# Calculate accuracy
best_modelf_accuracy_train = accuracy_score(y_trainf, best_modelf_train_preds)
print(f"Neural Network Model for Antifungal_Activity Accuracy on Training Data: {best_modelf_accuracy_train * 100:.2f}%")

# Calculate precision
best_modelf_precision_train = precision_score(y_trainf, best_modelf_train_preds)
print(f"Neural Network Model for Antifungal_Activity Precision on Training Data: {best_modelf_precision_train * 100:.2f}%")

# Calculate recall
best_modelf_recall_train = recall_score(y_trainf, best_modelf_train_preds)
print(f"Neural Network Model for Antifungal_Activity Recall on Training Data: {best_modelf_recall_train * 100:.2f}%")

# Calculate F1 score
best_modelf_f1_train = f1_score(y_trainf, best_modelf_train_preds)
print(f"Neural Network Model for Antifungal_Activity F1 score on Training Data: {best_modelf_f1_train * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_trainf, best_modelf_train_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Antifungal_Activity Neural Network Model on Training Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modelf_test_probs = best_modelf.predict(X_test)

# Convert probabilities to binary predictions
threshold = 0.5
best_modelf_test_preds = (best_modelf_test_probs > threshold).astype(int)

# Calculate accuracy
best_modelf_accuracy = accuracy_score(y_testf, best_modelf_test_preds)
print(f"Neural Network Model for Antifungal_Activity Accuracy on Test Data: {best_modelf_accuracy * 100:.2f}%")

# Calculate precision
best_modelf_precision = precision_score(y_testf, best_modelf_test_preds)
print(f"Neural Network Model for Antifungal_Activity Precision on Test Data: {best_modelf_precision * 100:.2f}%")

# Calculate recall
best_modelf_recall = recall_score(y_testf, best_modelf_test_preds)
print(f"Neural Network Model for Antifungal_Activity Recall on Test Data: {best_modelf_recall * 100:.2f}%")

# Calculate F1 score
best_modelf_f1 = f1_score(y_testf, best_modelf_test_preds)
print(f"Neural Network Model for Antifungal_Activity F1 score on Test Data: {best_modelf_f1 * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_testf, best_modelf_test_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Antifungal_Activity Neural Network Model on Test Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

# Retrieve the Best Hyperparameters for Antibacterial_Activity
best_hyperparametersb = tunerb.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters for Antibacterial_Activity:")
print(best_hyperparametersb.values)

# Get the Best Model
best_modelb = tunerb.get_best_models(num_models=1)[0]

# Use the Best Model
# evaluate the model on the test set
test_loss, test_accuracy = best_modelb.evaluate(X_test, y_testb)
print(f"Best Model Test Loss: {test_loss}")
print(f"Best Model Test Accuracy: {test_accuracy}")

best_modelb.summary()

plot_model(best_modelb, to_file='modelb.png', show_shapes=True, show_layer_names=True)

# To display the image in Colab:
Image('modelb.png')

from keras.metrics import Precision, Recall
import tensorflow as tf

# Compile the model without using custom f1 score
best_modelb.compile(
    optimizer=best_hyperparametersb.get('optimizer'),
    loss='binary_crossentropy',
    metrics=['accuracy', Precision(), Recall()]
)

from sklearn.metrics import f1_score
import numpy as np
import tensorflow as tf

# Custom callback to calculate and print validation F1 Score after each epoch
class F1ScoreCallback(tf.keras.callbacks.Callback):
    def __init__(self, validation_data):
        super(F1ScoreCallback, self).__init__()
        self.validation_data = validation_data
        self.val_f1s = []

    def on_epoch_end(self, epoch, logs=None):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        val_f1 = f1_score(val_targ, val_predict, average='macro')
        self.val_f1s.append(val_f1)
        print(f" — val_f1: {val_f1}")

# Initialize the callback
f1_callback = F1ScoreCallback(validation_data=(X_test, y_testb))

# Train the model and collect the history
history = best_modelb.fit(
    X_train, y_trainb,
    epochs=50,
    validation_data=(X_test, y_testb),
    callbacks=[f1_callback]
)

# Extract the training history (for precision, recall, etc.)
history_dict = history.history

# Print history keys to check the metrics available
print(history_dict.keys())

# Extract metrics from the training history
precision = history_dict.get('precision', [])  # Adjust based on printed keys
val_precision = history_dict.get('val_precision', [])
recall = history_dict.get('recall', [])
val_recall = history_dict.get('val_recall', [])
f1 = []  # Since we compute F1 manually in the callback
val_f1 = f1_callback.val_f1s

# Plot metrics
epochs = range(1, len(precision) + 1)

plt.figure(figsize=(15, 5))

# Precision plot
plt.subplot(1, 3, 1)
plt.plot(epochs, precision, label='Training Precision')
plt.plot(epochs, val_precision, label='Validation Precision')
plt.title('Training and Validation Precision for Antibacterial_Activity')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall plot
plt.subplot(1, 3, 2)
plt.plot(epochs, recall, label='Training Recall')
plt.plot(epochs, val_recall, label='Validation Recall')
plt.title('Training and Validation Recall for Antibacterial_Activity')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

# F1 Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs, val_f1, label='Validation F1 Score')
plt.title('Validation F1 Score for Antibacterial_Activity')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()

plt.tight_layout()
plt.show()

# Extract accuracy and loss values
acc = history_dict.get('accuracy', [])
val_acc = history_dict.get('val_accuracy', [])
loss = history_dict.get('loss', [])
val_loss = history_dict.get('val_loss', [])

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy for Antibacterial_Activity')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss for Antibacterial_Activity')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modelb_train_probs = best_modelb.predict(X_train)

# Convert probabilities to binary predictions
threshold = 0.5
best_modelb_train_preds = (best_modelb_train_probs > threshold).astype(int)

# Calculate accuracy
best_modelb_accuracy_train = accuracy_score(y_trainb, best_modelb_train_preds)
print(f"Neural Network Model for Antibacterial_Activity Accuracy on Training Data: {best_modelb_accuracy_train * 100:.2f}%")

# Calculate precision
best_modelb_precision_train = precision_score(y_trainb, best_modelb_train_preds)
print(f"Neural Network Model for Antibacterial_Activity Precision on Training Data: {best_modelb_precision_train * 100:.2f}%")

# Calculate recall
best_modelb_recall_train = recall_score(y_trainb, best_modelb_train_preds)
print(f"Neural Network Model for Antibacterial_Activity Recall on Training Data: {best_modelb_recall_train * 100:.2f}%")

# Calculate F1 score
best_modelb_f1_train = f1_score(y_trainb, best_modelb_train_preds)
print(f"Neural Network Model for Antibacterial_Activity F1 score on Training Data: {best_modelb_f1_train * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_trainb, best_modelb_train_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Antibacterial_Activity Neural Network Model on Training Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modelb_test_probs = best_modelb.predict(X_test)

# Convert probabilities to binary predictions
threshold = 0.5
best_modelb_test_preds = (best_modelb_test_probs > threshold).astype(int)

# Calculate accuracy
best_modelb_accuracy = accuracy_score(y_testb, best_modelb_test_preds)
print(f"Neural Network Model for Antifungal_Activity Accuracy on Test Data: {best_modelb_accuracy * 100:.2f}%")

# Calculate precision
best_modelb_precision = precision_score(y_testb, best_modelb_test_preds)
print(f"Neural Network Model for Antifungal_Activity Precision on Test Data: {best_modelb_precision * 100:.2f}%")

# Calculate recall
best_modelb_recall = recall_score(y_testb, best_modelb_test_preds)
print(f"Neural Network Model for Antifungal_Activity Recall on Test Data: {best_modelb_recall * 100:.2f}%")

# Calculate F1 score
best_modelb_f1 = f1_score(y_testb, best_modelb_test_preds)
print(f"Neural Network Model for Antifungal_Activity F1 score on Test Data: {best_modelb_f1 * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_testb, best_modelb_test_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Antifungal_Activity Neural Network Model on Test Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

# Retrieve the Best Hyperparameters for Antioxidant_Activity
best_hyperparameterso = tunero.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters for Antioxidant_Activity:")
print(best_hyperparameterso.values)

# Get the Best Model for Antioxidant_Activity
best_modelo = tunero.get_best_models(num_models=1)[0]

# Use the Best Model
# evaluate the model on the test set
test_loss, test_accuracy = best_modelo.evaluate(X_test, y_testo)
print(f"Best Model Test Loss: {test_loss}")
print(f"Best Model Test Accuracy: {test_accuracy}")
best_modelo.save('best_modelo.h5')

best_modelo.summary()

plot_model(best_modelo, to_file='modelo.png', show_shapes=True, show_layer_names=True)

# To display the image in Colab:
Image('modelo.png')

# Compile the model without using custom f1 score
best_modelo.compile(
    optimizer=best_hyperparameterso.get('optimizer'),
    loss='binary_crossentropy',
    metrics=['accuracy', Precision(), Recall()]
)

from sklearn.metrics import f1_score
import numpy as np
import tensorflow as tf

# Custom callback to calculate and print validation F1 Score after each epoch
class F1ScoreCallback(tf.keras.callbacks.Callback):
    def __init__(self, validation_data):
        super(F1ScoreCallback, self).__init__()
        self.validation_data = validation_data
        self.val_f1s = []

    def on_epoch_end(self, epoch, logs=None):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        val_f1 = f1_score(val_targ, val_predict, average='macro')
        self.val_f1s.append(val_f1)
        print(f" — val_f1: {val_f1}")

# Initialize the callback
f1_callback = F1ScoreCallback(validation_data=(X_test, y_testo))

# Train the model and collect the history
history = best_modelo.fit(
    X_train, y_traino,
    epochs=50,
    validation_data=(X_test, y_testo),
    callbacks=[f1_callback]
)

# Extract the training history (for precision, recall, etc.)
history_dict = history.history

# Print history keys to check the metrics available
print(history_dict.keys())

# Extract metrics from the training history
precision = history_dict.get('precision', [])  # Adjust based on printed keys
val_precision = history_dict.get('val_precision', [])
recall = history_dict.get('recall', [])
val_recall = history_dict.get('val_recall', [])
f1 = []  # Since we compute F1 manually in the callback
val_f1 = f1_callback.val_f1s

# Plot metrics
epochs = range(1, len(precision) + 1)

plt.figure(figsize=(15, 5))

# Precision plot
plt.subplot(1, 3, 1)
plt.plot(epochs, precision, label='Training Precision')
plt.plot(epochs, val_precision, label='Validation Precision')
plt.title('Training and Validation Precision for Antioxidant_Activity')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall plot
plt.subplot(1, 3, 2)
plt.plot(epochs, recall, label='Training Recall')
plt.plot(epochs, val_recall, label='Validation Recall')
plt.title('Training and Validation Recall for Antioxidant_Activity')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

# F1 Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs, val_f1, label='Validation F1 Score')
plt.title('Validation F1 Score for Antioxidant_Activity')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()

plt.tight_layout()
plt.show()

# Extract accuracy and loss values
acc = history_dict.get('accuracy', [])
val_acc = history_dict.get('val_accuracy', [])
loss = history_dict.get('loss', [])
val_loss = history_dict.get('val_loss', [])

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy for Antioxidant_Activity')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss for Antioxidant_Activity')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modelo_train_probs = best_modelo.predict(X_train)

# Convert probabilities to binary predictions
threshold = 0.5
best_modelo_train_preds = (best_modelo_train_probs > threshold).astype(int)

# Calculate accuracy
best_modelo_accuracy_train = accuracy_score(y_traino, best_modelo_train_preds)
print(f"Neural Network Model for Antioxidant_Activity Accuracy on Training Data: {best_modelo_accuracy_train * 100:.2f}%")

# Calculate precision
best_modelo_precision_train = precision_score(y_traino, best_modelo_train_preds)
print(f"Neural Network Model for Antioxidant_Activity Precision on Training Data: {best_modelo_precision_train * 100:.2f}%")

# Calculate recall
best_modelo_recall_train = recall_score(y_traino, best_modelo_train_preds)
print(f"Neural Network Model for Antioxidant_Activity Recall on Training Data: {best_modelo_recall_train * 100:.2f}%")

# Calculate F1 score
best_modelo_f1_train = f1_score(y_traino, best_modelo_train_preds)
print(f"Neural Network Model for Antioxidant_Activity F1 score on Training Data: {best_modelo_f1_train * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_traino, best_modelo_train_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Antioxidant_Activity Neural Network Model on Training Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modelo_test_probs = best_modelo.predict(X_test)

# Convert probabilities to binary predictions
threshold = 0.5
best_modelo_test_preds = (best_modelo_test_probs > threshold).astype(int)

# Calculate accuracy
best_modelo_accuracy = accuracy_score(y_testo, best_modelo_test_preds)
print(f"Neural Network Model for Antioxidant_Activity Accuracy on Test Data: {best_modelo_accuracy * 100:.2f}%")

# Calculate precision
best_modelo_precision = precision_score(y_testo, best_modelo_test_preds)
print(f"Neural Network Model for Antioxidant_Activity Precision on Test Data: {best_modelo_precision * 100:.2f}%")

# Calculate recall
best_modelo_recall = recall_score(y_testo, best_modelo_test_preds)
print(f"Neural Network Model for Antioxidant_Activity Recall on Test Data: {best_modelo_recall * 100:.2f}%")

# Calculate F1 score
best_modelo_f1 = f1_score(y_testo, best_modelo_test_preds)
print(f"Neural Network Model for Antioxidant_Activity F1 score on Test Data: {best_modelo_f1 * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_testo, best_modelo_test_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Antioxidant_Activity Neural Network Model on Test Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

# Retrieve the Best Hyperparameters for Anti_Inflammatory_Activity
best_hyperparametersi = tuneri.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters for Anti_Inflammatory_Activity:")
print(best_hyperparametersi.values)

# Get the Best Model
best_modeli = tuneri.get_best_models(num_models=1)[0]

# Use the Best Model
# evaluate the model on the test set
test_loss, test_accuracy = best_modeli.evaluate(X_test, y_testi)
print(f"Best Model Test Loss: {test_loss}")
print(f"Best Model Test Accuracy: {test_accuracy}")

best_modeli.summary()

from tensorflow.keras.utils import plot_model
from IPython.display import Image
plot_model(best_modeli, to_file='modeli.png', show_shapes=True, show_layer_names=True)

# To display the image in Colab:
Image('modeli.png')

# Compile the model without using custom f1 score
best_modeli.compile(
    optimizer=best_hyperparametersi.get('optimizer'),
    loss='binary_crossentropy',
    metrics=['accuracy', Precision(), Recall()]
)

from sklearn.metrics import f1_score
import numpy as np
import tensorflow as tf

# Custom callback to calculate and print validation F1 Score after each epoch
class F1ScoreCallback(tf.keras.callbacks.Callback):
    def __init__(self, validation_data):
        super(F1ScoreCallback, self).__init__()
        self.validation_data = validation_data
        self.val_f1s = []

    def on_epoch_end(self, epoch, logs=None):
        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()
        val_targ = self.validation_data[1]
        val_f1 = f1_score(val_targ, val_predict, average='macro')
        self.val_f1s.append(val_f1)
        print(f" — val_f1: {val_f1}")

# Initialize the callback
f1_callback = F1ScoreCallback(validation_data=(X_test, y_testi))

# Train the model and collect the history
history = best_modeli.fit(
    X_train, y_traini,
    epochs=50,
    validation_data=(X_test, y_testi),
    callbacks=[f1_callback]
)

# Extract the training history (for precision, recall, etc.)
history_dict = history.history

# Print history keys to check the metrics available
print(history_dict.keys())

# Extract metrics from the training history
precision = history_dict.get('precision', [])  # Adjust based on printed keys
val_precision = history_dict.get('val_precision', [])
recall = history_dict.get('recall', [])
val_recall = history_dict.get('val_recall', [])
f1 = []  # Since we compute F1 manually in the callback
val_f1 = f1_callback.val_f1s

# Plot metrics
epochs = range(1, len(precision) + 1)

plt.figure(figsize=(15, 5))

# Precision plot
plt.subplot(1, 3, 1)
plt.plot(epochs, precision, label='Training Precision')
plt.plot(epochs, val_precision, label='Validation Precision')
plt.title('Training and Validation Precision for Anti_Inflammatory_Activity')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()

# Recall plot
plt.subplot(1, 3, 2)
plt.plot(epochs, recall, label='Training Recall')
plt.plot(epochs, val_recall, label='Validation Recall')
plt.title('Training and Validation Recall for Anti_Inflammatory_Activity')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()

# F1 Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs, val_f1, label='Validation F1 Score')
plt.title('Validation F1 Score for Anti_Inflammatory_Activity')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()

plt.tight_layout()
plt.show()

# Extract accuracy and loss values
acc = history_dict.get('accuracy', [])
val_acc = history_dict.get('val_accuracy', [])
loss = history_dict.get('loss', [])
val_loss = history_dict.get('val_loss', [])

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy for Anti_Inflammatory_Activity')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss for Anti_Inflammatory_Activity')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modeli_train_probs = best_modeli.predict(X_train)

# Convert probabilities to binary predictions
threshold = 0.5
best_modeli_train_preds = (best_modeli_train_probs > threshold).astype(int)

# Calculate accuracy
best_modeli_accuracy_train = accuracy_score(y_traini, best_modeli_train_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity Accuracy on Training Data: {best_modeli_accuracy_train * 100:.2f}%")

# Calculate precision
best_modeli_precision_train = precision_score(y_traini, best_modeli_train_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity Precision on Training Data: {best_modeli_precision_train * 100:.2f}%")

# Calculate recall
best_modeli_recall_train = recall_score(y_traini, best_modeli_train_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity Recall on Training Data: {best_modeli_recall_train * 100:.2f}%")

# Calculate F1 score
best_modeli_f1_train = f1_score(y_traini, best_modeli_train_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity F1 score on Training Data: {best_modeli_f1_train * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_traini, best_modeli_train_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Anti_Inflammatory_Activity Neural Network Model on Training Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict probabilities
best_modeli_test_probs = best_modeli.predict(X_test)

# Convert probabilities to binary predictions
threshold = 0.5
best_modeli_test_preds = (best_modeli_test_probs > threshold).astype(int)

# Calculate accuracy
best_modeli_accuracy = accuracy_score(y_testi, best_modeli_test_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity Accuracy on Test Data: {best_modeli_accuracy * 100:.2f}%")

# Calculate precision
best_modeli_precision = precision_score(y_testi, best_modeli_test_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity Precision on Test Data: {best_modeli_precision * 100:.2f}%")

# Calculate recall
best_modeli_recall = recall_score(y_testi, best_modeli_test_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity Recall on Test Data: {best_modeli_recall * 100:.2f}%")

# Calculate F1 score
best_modeli_f1 = f1_score(y_testi, best_modeli_test_preds)
print(f"Neural Network Model for Anti_Inflammatory_Activity F1 score on Test Data: {best_modeli_f1 * 100:.2f}%")

# Visualising Confusion Matrix
cm = confusion_matrix(y_testi, best_modeli_test_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix for Anti_Inflammatory_Activity Neural Network Model on Test Data", fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()

"""# **MODEL-2: MULTITASK NEURAL NETWORK**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization
from tensorflow.keras.optimizers import Adam
from keras_tuner import HyperModel
from sklearn.model_selection import train_test_split
import keras_tuner as kt

# Define the multitask model for hyperparameter tuning
def build_multitask_model(hp):
    inputs = tf.keras.Input(shape=(X_train.shape[1],))

    # Shared layers (tunable number of layers and units)
    x = inputs
    for i in range(hp.Int('num_shared_layers', min_value=1, max_value=10)):
        x = Dense(hp.Int('units_shared_'+str(i), min_value=8, max_value=128, step=8), activation=None)(x)

        if hp.Boolean('batch_norm_shared_'+str(i)):
            x = BatchNormalization()(x)

        x = Activation(hp.Choice('activation_shared_'+str(i), values=['relu', 'tanh', 'sigmoid']))(x)
        x = Dropout(hp.Choice('dropout_shared_'+str(i), values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))(x)

    # Task-specific output layers
    antifungal_output = Dense(1, activation='sigmoid', name='antifungal')(x)
    antibacterial_output = Dense(1, activation='sigmoid', name='antibacterial')(x)
    antioxidant_output = Dense(1, activation='sigmoid', name='antioxidant')(x)
    anti_inflammatory_output = Dense(1, activation='sigmoid', name='anti_inflammatory')(x)

    # Define the model
    model = tf.keras.Model(inputs=inputs, outputs=[antifungal_output, antibacterial_output, antioxidant_output, anti_inflammatory_output])

    # Compile the model with a tunable optimizer
    model.compile(
        optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd', 'nadam','adadelta']),
        loss='binary_crossentropy',
        metrics={'antifungal': 'accuracy', 'antibacterial': 'accuracy', 'antioxidant': 'accuracy', 'anti_inflammatory': 'accuracy'})

    return model

# Define the custom aggregate accuracy metric
class AggregateAccuracy(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        # Aggregate the accuracy of all tasks
        acc_antifungal = logs.get('val_antifungal_accuracy', 0)
        acc_antibacterial = logs.get('val_antibacterial_accuracy', 0)
        acc_antioxidant = logs.get('val_antioxidant_accuracy', 0)
        acc_anti_inflammatory = logs.get('val_anti_inflammatory_accuracy', 0)

        # Calculate the average accuracy across tasks
        aggregate_accuracy = (acc_antifungal + acc_antibacterial + acc_antioxidant + acc_anti_inflammatory) / 4.0

        # Log the custom metric
        logs['val_aggregate_accuracy'] = aggregate_accuracy
        print(f"Epoch {epoch+1}: val_aggregate_accuracy = {aggregate_accuracy:.4f}")

# Early stopping mechanism to monitor model performance
from tensorflow.keras.callbacks import EarlyStopping

# Define the early stopping callback
early_stopping = EarlyStopping(
    monitor='val_aggregate_accuracy',  # Monitor the custom metric
    patience=5,               # Number of epochs to wait for improvement
    restore_best_weights=True # Restore the model weights from the epoch with the best value of the monitored metric
)

# Initialize Keras Tuner with custom metric
tuner = kt.RandomSearch(
    build_multitask_model,
    objective=kt.Objective('val_aggregate_accuracy', direction='max'),  # Use custom aggregate accuracy
    max_trials=50,
    directory='mydir4',
    project_name='multitask_tuning4'
)

# Start hyperparameter search
tuner.search(
    X_train,
    [y_trainf, y_trainb, y_traino, y_traini],
    validation_data=(X_test, [y_testf, y_testb, y_testo, y_testi]),
    epochs=100,
    callbacks=[early_stopping, AggregateAccuracy()]  # Include the custom metric in the callbacks
)

# Retrieve the Best Hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters for Multitask Neural Network Model:")
print(best_hyperparameters.values)

# Get the best model
best_model = tuner.get_best_models(num_models=1)[0]

# Evaluate the best model on the test set
test_results = best_model.evaluate(X_test,
                                   [y_testf, y_testb, y_testo, y_testi])

print("Test Results:", test_results)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict on training data (multitask model returns predictions for each task)
best_model_train_preds = best_model.predict(X_train)

# Predict on test data
best_model_test_preds = best_model.predict(X_test)

# Assuming the model returns predictions in the order of tasks
task_names = ['Anti_Fungal_Activity', 'Anti_Bacterial_Activity', 'Anti_Oxidant_Activity', 'Anti_Inflammatory_Activity']
task_labels_train = {
    'Anti_Fungal_Activity': y_trainf,
    'Anti_Bacterial_Activity': y_trainb,
    'Anti_Oxidant_Activity': y_traino,
    'Anti_Inflammatory_Activity': y_traini
}

task_labels_test = {
    'Anti_Fungal_Activity': y_testf,
    'Anti_Bacterial_Activity': y_testb,
    'Anti_Oxidant_Activity': y_testo,
    'Anti_Inflammatory_Activity': y_testi
}

# Define the threshold for binary classification
threshold = 0.5

# Function to evaluate a task on given predictions and true labels
def evaluate_task(task_name, task_true, task_preds):
    # Convert probabilities to binary predictions
    task_binary_preds = (task_preds > threshold).astype(int)

    # Calculate metrics
    accuracy = accuracy_score(task_true, task_binary_preds)
    precision = precision_score(task_true, task_binary_preds)
    recall = recall_score(task_true, task_binary_preds)
    f1 = f1_score(task_true, task_binary_preds)

    # Print results
    print(f"{task_name} Accuracy: {accuracy * 100:.2f}%")
    print(f"{task_name} Precision: {precision * 100:.2f}%")
    print(f"{task_name} Recall: {recall * 100:.2f}%")
    print(f"{task_name} F1 score: {f1 * 100:.2f}%")

    # Visualize confusion matrix
    cm = confusion_matrix(task_true, task_binary_preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f"Confusion Matrix for {task_name}", fontsize=18)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()

# Loop over each task for training and test data
for i, task_name in enumerate(task_names):
    print(f"Evaluating Task: {task_name}")

    # Training evaluation
    print("\n--- Training Data ---")
    task_train_preds = best_model_train_preds[i]  # Ensure the correct prediction order
    task_train_true = task_labels_train[task_name]  # Ensure correct label order
    evaluate_task(task_name, task_train_true, task_train_preds)

    # Test evaluation
    print("\n--- Test Data ---")
    task_test_preds = best_model_test_preds[i]  # Ensure the correct prediction order
    task_test_true = task_labels_test[task_name]  # Ensure correct label order
    evaluate_task(task_name, task_test_true, task_test_preds)

from tensorflow.keras.utils import plot_model

# Assuming you have already defined and compiled your model
plot_model(best_model, to_file='multitask_model.png', show_shapes=True, show_layer_names=True)

# To display the image in Colab:
from IPython.display import Image
Image('multitask_model.png')

"""# **Model-3 TABNET FOR EVERY LABEL**"""

pip install pytorch-tabnet

"""# **3.A TabNET For Predicting Antibacterial_Activity**"""

import numpy as np
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch

# Ensure that X_train and X_test are numpy arrays, and y_train and y_test are numpy arrays
X_train = X_train.values
X_test = X_test.values
y_trainb = y_trainb.values
y_testb = y_testb.values

# Define and initialize the TabNet model
tabnet_clf = TabNetClassifier()

# Train TabNet on your Anti_Bacterial_Activity data (or any other target label)
history_single = tabnet_clf.fit(
    X_train, y_trainb,
    eval_set=[(X_test, y_testb)],
    eval_name=["val"],
    eval_metric=["accuracy", "logloss"],  # Removed unsupported metrics
    max_epochs=100,
    patience=10,     # Early stopping if no improvement
    batch_size=1024,
    virtual_batch_size=128
)

# Predict for train and test set
y_predb_train = tabnet_clf.predict(X_train)
y_predb_test = tabnet_clf.predict(X_test)

# Calculate performance metrics on training data
accuracy_train = accuracy_score(y_trainb, y_predb_train)
precision_train = precision_score(y_trainb, y_predb_train)
recall_train = recall_score(y_trainb, y_predb_train)
f1_train = f1_score(y_trainb, y_predb_train)

print(f"TabNet Model Accuracy on Training Data for Anti_Bacterial_Activity: {accuracy_train * 100:.2f}%")
print(f"TabNet Model Precision on Training Data for Anti_Bacterial_Activity: {precision_train * 100:.2f}%")
print(f"TabNet Model Recall on Training Data for Anti_Bacterial_Activity: {recall_train * 100:.2f}%")
print(f"TabNet Model F1 Score on Training Data for Anti_Bacterial_Activity: {f1_train * 100:.2f}%")

# Calculate performance metrics on test data
accuracy_test = accuracy_score(y_testb, y_predb_test)
precision_test = precision_score(y_testb, y_predb_test)
recall_test = recall_score(y_testb, y_predb_test)
f1_test = f1_score(y_testb, y_predb_test)

print(f"TabNet Model Accuracy on Test Data for Anti_Bacterial_Activity: {accuracy_test * 100:.2f}%")
print(f"TabNet Model Precision on Test Data for Anti_Bacterial_Activity: {precision_test * 100:.2f}%")
print(f"TabNet Model Recall on Test Data for Anti_Bacterial_Activity: {recall_test * 100:.2f}%")
print(f"TabNet Model F1 Score on Test Data for Anti_Bacterial_Activity: {f1_test * 100:.2f}%")

# Save the model if necessary
tabnet_clf.save_model("tabnet_model_anti_bacterial")

# Optionally, load the model in future if required
# tabnet_clf.load_model("tabnet_model_anti_bacterial.zip")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_trainb, y_predb_train)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Anti_Bacterial_Activity (Single-Task TabNet) - Training Data", fontsize=16)
plt.show()

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_testb, y_predb_test)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Anti_Bacterial_Activity (Single-Task TabNet) - Test Data", fontsize=16)
plt.show()

!pip install torchviz

from torchviz import make_dot
import torch

# Input shape
input_shape = X_train.shape[1]

# Generate a random input with the same shape as your training data
x = torch.randn(1, input_shape)

# Pass the input through the model's network (forward pass)
y = tabnet_clf.network(x)  # Access the network attribute directly for forward pass

# Create a visualization of the computational graph
# Since 'named_parameters' is not available, we'll pass the network's state_dict instead
make_dot(y, params=dict(tabnet_clf.network.state_dict())).render("tabnet_clf", format="png")

# Display the generated model architecture
from IPython.display import Image
Image('tabnet_clf.png')

# Inspect the history object to see available keys
print(tabnet_clf.history)

import matplotlib.pyplot as plt

# Function to plot learning curves for a metric
def plot_learning_curves(history, val_metric_name, metric_label, title):
    val_metric = history[val_metric_name] if val_metric_name in history else None

    plt.figure(figsize=(10, 6))

    if val_metric is not None:
        plt.plot(val_metric, label=f'Validation {metric_label}')

    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel(metric_label)
    plt.legend()
    plt.grid(True)
    plt.show()

# Extract history
history = tabnet_clf.history

# Accessing metrics directly from history
val_accuracy = history['val_accuracy']
val_logloss = history['val_logloss']
lr = history['lr']

# Plot Validation Accuracy
plt.figure(figsize=(10, 6))
plt.plot(val_accuracy, label='Validation Accuracy')
plt.title("Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# Plot Validation Loss
plt.figure(figsize=(10, 6))
plt.plot(val_logloss, label='Validation Loss')
plt.title("Validation Loss")
plt.xlabel('Epochs')
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Optionally, plot the learning rate evolution
plt.figure(figsize=(10, 6))
plt.plot(lr, label='Learning Rate')
plt.title("Learning Rate Over Epochs")
plt.xlabel('Epochs')
plt.ylabel("Learning Rate")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Initialize lists to store metrics for each epoch
precision_train_list = []
recall_train_list = []
f1_train_list = []
precision_val_list = []
recall_val_list = []
f1_val_list = []
accuracy_train_list = []

# Compute metrics for both training and validation sets for each epoch
for epoch in range(len(tabnet_clf.history['val_logloss'])):  # Use validation loss as a reference
    # Get predictions from the model on the training set
    y_pred_train = tabnet_clf.predict(X_train)

    # Calculate training metrics
    precision_train = precision_score(y_trainb, y_pred_train)
    recall_train = recall_score(y_trainb, y_pred_train)
    f1_train = f1_score(y_trainb, y_pred_train)
    accuracy_train = accuracy_score(y_trainb, y_pred_train)

    # Append training metrics to lists
    precision_train_list.append(precision_train)
    recall_train_list.append(recall_train)
    f1_train_list.append(f1_train)
    accuracy_train_list.append(accuracy_train)

    # Get predictions from the model on the validation set
    y_pred_val = tabnet_clf.predict(X_test)

    # Calculate validation metrics
    precision_val = precision_score(y_testb, y_pred_val)
    recall_val = recall_score(y_testb, y_pred_val)
    f1_val = f1_score(y_testb, y_pred_val)

    # Append validation metrics to lists
    precision_val_list.append(precision_val)
    recall_val_list.append(recall_val)
    f1_val_list.append(f1_val)

# Plot Accuracy
plt.figure(figsize=(10, 6))
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), tabnet_clf.history['val_accuracy'], label='Validation Accuracy')
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), accuracy_train_list, label='Training Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Precision
plt.figure(figsize=(10, 6))
plt.plot(range(len(precision_train_list)), precision_train_list, label='Training Precision')
plt.plot(range(len(precision_val_list)), precision_val_list, label='Validation Precision')
plt.title('Precision over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)
plt.show()

# Plot Recall
plt.figure(figsize=(10, 6))
plt.plot(range(len(recall_train_list)), recall_train_list, label='Training Recall')
plt.plot(range(len(recall_val_list)), recall_val_list, label='Validation Recall')
plt.title('Recall over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()
plt.grid(True)
plt.show()

# Plot F1 Score
plt.figure(figsize=(10, 6))
plt.plot(range(len(f1_train_list)), f1_train_list, label='Training F1 Score')
plt.plot(range(len(f1_val_list)), f1_val_list, label='Validation F1 Score')
plt.title('F1 Score over Epochs')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True)
plt.show()

"""# **3.B TabNET for predicting Anti_fungal Activity**"""

import numpy as np
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch

# Ensure that X_train and X_test are numpy arrays, and y_train and y_test are numpy arrays
'''X_train = X_train.values
X_test = X_test.values'''
y_trainf = y_trainf.values
y_testf = y_testf.values

# Define and initialize the TabNet model
tabnet_clf = TabNetClassifier()

# Train TabNet on your Anti_Bacterial_Activity data (or any other target label)
history_single = tabnet_clf.fit(
    X_train, y_trainf,
    eval_set=[(X_test, y_testf)],
    eval_name=["val"],
    eval_metric=["accuracy", "logloss"],  # Removed unsupported metrics
    max_epochs=100,
    patience=10,     # Early stopping if no improvement
    batch_size=1024,
    virtual_batch_size=128
)

# Predict for train and test set
y_predf_train = tabnet_clf.predict(X_train)
y_predf_test = tabnet_clf.predict(X_test)

# Calculate performance metrics on training data
accuracy_train = accuracy_score(y_trainf, y_predf_train)
precision_train = precision_score(y_trainf, y_predf_train)
recall_train = recall_score(y_trainf, y_predf_train)
f1_train = f1_score(y_trainf, y_predf_train)

print(f"TabNet Model Accuracy on Training Data for Anti_fungal_Activity: {accuracy_train * 100:.2f}%")
print(f"TabNet Model Precision on Training Data for Anti_fungal_Activity: {precision_train * 100:.2f}%")
print(f"TabNet Model Recall on Training Data for Anti_fungal_Activity: {recall_train * 100:.2f}%")
print(f"TabNet Model F1 Score on Training Data for Anti_fungal_Activity: {f1_train * 100:.2f}%")

# Calculate performance metrics on test data
accuracy_test = accuracy_score(y_testf, y_predf_test)
precision_test = precision_score(y_testf, y_predf_test)
recall_test = recall_score(y_testf, y_predf_test)
f1_test = f1_score(y_testf, y_predf_test)

print(f"TabNet Model Accuracy on Test Data for Anti_fungal_Activity: {accuracy_test * 100:.2f}%")
print(f"TabNet Model Precision on Test Data for Anti_fungal_Activity: {precision_test * 100:.2f}%")
print(f"TabNet Model Recall on Test Data for Anti_fungal_Activity: {recall_test * 100:.2f}%")
print(f"TabNet Model F1 Score on Test Data for Anti_fungal_Activity: {f1_test * 100:.2f}%")

# Save the model if necessary
tabnet_clf.save_model("tabnet_model_anti_fungal")

# Optionally, load the model in future if required
# tabnet_clf.load_model("tabnet_model_anti_fungal.zip")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_trainf, y_predf_train)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Anti_Fungal_Activity (Single-Task TabNet) - Training Data", fontsize=16)
plt.show()

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_testf, y_predf_test)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Anti_Fungal_Activity (Single-Task TabNet) - Test Data", fontsize=16)
plt.show()

from torchviz import make_dot
import torch

# Input shape
input_shape = X_train.shape[1]

# Generate a random input with the same shape as your training data
x = torch.randn(1, input_shape)

# Pass the input through the model's network (forward pass)
y = tabnet_clf.network(x)  # Access the network attribute directly for forward pass

# Create a visualization of the computational graph
# Since 'named_parameters' is not available, we'll pass the network's state_dict instead
make_dot(y, params=dict(tabnet_clf.network.state_dict())).render("tabnet_clf_antifungal", format="png")

# Display the generated model architecture
from IPython.display import Image
Image('tabnet_clf_antifungal.png')

import matplotlib.pyplot as plt

# Function to plot learning curves for a metric
def plot_learning_curves(history, val_metric_name, metric_label, title):
    val_metric = history[val_metric_name] if val_metric_name in history else None

    plt.figure(figsize=(10, 6))

    if val_metric is not None:
        plt.plot(val_metric, label=f'Validation {metric_label}')

    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel(metric_label)
    plt.legend()
    plt.grid(True)
    plt.show()

# Extract history
history = tabnet_clf.history

# Accessing metrics directly from history
val_accuracy = history['val_accuracy']
val_logloss = history['val_logloss']
lr = history['lr']

# Plot Validation Accuracy
plt.figure(figsize=(10, 6))
plt.plot(val_accuracy, label='Validation Accuracy')
plt.title("Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# Plot Validation Loss
plt.figure(figsize=(10, 6))
plt.plot(val_logloss, label='Validation Loss')
plt.title("Validation Loss")
plt.xlabel('Epochs')
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Optionally, plot the learning rate evolution
plt.figure(figsize=(10, 6))
plt.plot(lr, label='Learning Rate')
plt.title("Learning Rate Over Epochs")
plt.xlabel('Epochs')
plt.ylabel("Learning Rate")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Initialize lists to store metrics for each epoch
precision_train_list = []
recall_train_list = []
f1_train_list = []
precision_val_list = []
recall_val_list = []
f1_val_list = []
accuracy_train_list = []

# Compute metrics for both training and validation sets for each epoch
for epoch in range(len(tabnet_clf.history['val_logloss'])):  # Use validation loss as a reference
    # Get predictions from the model on the training set
    y_pred_train = tabnet_clf.predict(X_train)

    # Calculate training metrics
    precision_train = precision_score(y_trainf, y_pred_train)
    recall_train = recall_score(y_trainf, y_pred_train)
    f1_train = f1_score(y_trainf, y_pred_train)
    accuracy_train = accuracy_score(y_trainf, y_pred_train)

    # Append training metrics to lists
    precision_train_list.append(precision_train)
    recall_train_list.append(recall_train)
    f1_train_list.append(f1_train)
    accuracy_train_list.append(accuracy_train)

    # Get predictions from the model on the validation set
    y_pred_val = tabnet_clf.predict(X_test)

    # Calculate validation metrics
    precision_val = precision_score(y_testf, y_pred_val)
    recall_val = recall_score(y_testf, y_pred_val)
    f1_val = f1_score(y_testf, y_pred_val)

    # Append validation metrics to lists
    precision_val_list.append(precision_val)
    recall_val_list.append(recall_val)
    f1_val_list.append(f1_val)

# Plot Accuracy
plt.figure(figsize=(10, 6))
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), tabnet_clf.history['val_accuracy'], label='Validation Accuracy')
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), accuracy_train_list, label='Training Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Precision
plt.figure(figsize=(10, 6))
plt.plot(range(len(precision_train_list)), precision_train_list, label='Training Precision')
plt.plot(range(len(precision_val_list)), precision_val_list, label='Validation Precision')
plt.title('Precision over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)
plt.show()

# Plot Recall
plt.figure(figsize=(10, 6))
plt.plot(range(len(recall_train_list)), recall_train_list, label='Training Recall')
plt.plot(range(len(recall_val_list)), recall_val_list, label='Validation Recall')
plt.title('Recall over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()
plt.grid(True)
plt.show()

# Plot F1 Score
plt.figure(figsize=(10, 6))
plt.plot(range(len(f1_train_list)), f1_train_list, label='Training F1 Score')
plt.plot(range(len(f1_val_list)), f1_val_list, label='Validation F1 Score')
plt.title('F1 Score over Epochs')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True)
plt.show()

"""# **3.C TabNET for predicting Antioxidant_Activity**"""

import numpy as np
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch

# Ensure that X_train and X_test are numpy arrays, and y_train and y_test are numpy arrays
'''X_train = X_train.values
X_test = X_test.values'''
y_traino = y_traino.values
y_testo = y_testo.values

# Define and initialize the TabNet model
tabnet_clf = TabNetClassifier()

# Train TabNet on your Anti_Bacterial_Activity data (or any other target label)
history_single = tabnet_clf.fit(
    X_train, y_traino,
    eval_set=[(X_test, y_testo)],
    eval_name=["val"],
    eval_metric=["accuracy", "logloss"],  # Removed unsupported metrics
    max_epochs=100,
    patience=10,     # Early stopping if no improvement
    batch_size=1024,
    virtual_batch_size=128
)

# Predict for train and test set
y_predo_train = tabnet_clf.predict(X_train)
y_predo_test = tabnet_clf.predict(X_test)

# Calculate performance metrics on training data
accuracy_train = accuracy_score(y_traino, y_predo_train)
precision_train = precision_score(y_traino, y_predo_train)
recall_train = recall_score(y_traino, y_predo_train)
f1_train = f1_score(y_traino, y_predo_train)

print(f"TabNet Model Accuracy on Training Data for Antioxidant_Activity: {accuracy_train * 100:.2f}%")
print(f"TabNet Model Precision on Training Data for Antioxidant_Activity: {precision_train * 100:.2f}%")
print(f"TabNet Model Recall on Training Data for Antioxidant_Activity: {recall_train * 100:.2f}%")
print(f"TabNet Model F1 Score on Training Data for Antioxidant_Activity: {f1_train * 100:.2f}%")

# Calculate performance metrics on test data
accuracy_test = accuracy_score(y_testo, y_predo_test)
precision_test = precision_score(y_testo, y_predo_test)
recall_test = recall_score(y_testo, y_predo_test)
f1_test = f1_score(y_testo, y_predo_test)

print(f"TabNet Model Accuracy on Test Data for Antioxidant_Activity: {accuracy_test * 100:.2f}%")
print(f"TabNet Model Precision on Test Data for Antioxidant_Activity: {precision_test * 100:.2f}%")
print(f"TabNet Model Recall on Test Data for Antioxidant_Activity: {recall_test * 100:.2f}%")
print(f"TabNet Model F1 Score on Test Data for Antioxidant_Activity: {f1_test * 100:.2f}%")

# Save the model if necessary
tabnet_clf.save_model("tabnet_model_antioxidant")

# Optionally, load the model in future if required
# tabnet_clf.load_model("tabnet_model_anti_bacterial.zip")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_traino, y_predo_train)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Antioxidant_Activity (Single-Task TabNet) - Training Data", fontsize=16)
plt.show()

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_testo, y_predo_test)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Antioxidant_Activity (Single-Task TabNet) - Test Data", fontsize=16)
plt.show()

from torchviz import make_dot
import torch

# Input shape
input_shape = X_train.shape[1]

# Generate a random input with the same shape as your training data
x = torch.randn(1, input_shape)

# Pass the input through the model's network (forward pass)
y = tabnet_clf.network(x)  # Access the network attribute directly for forward pass

# Create a visualization of the computational graph
# Since 'named_parameters' is not available, we'll pass the network's state_dict instead
make_dot(y, params=dict(tabnet_clf.network.state_dict())).render("tabnet_clf_antioxidant", format="png")

# Display the generated model architecture
from IPython.display import Image
Image('tabnet_clf_antioxidant.png')

import matplotlib.pyplot as plt

# Function to plot learning curves for a metric
def plot_learning_curves(history, val_metric_name, metric_label, title):
    val_metric = history[val_metric_name] if val_metric_name in history else None

    plt.figure(figsize=(10, 6))

    if val_metric is not None:
        plt.plot(val_metric, label=f'Validation {metric_label}')

    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel(metric_label)
    plt.legend()
    plt.grid(True)
    plt.show()

# Extract history
history = tabnet_clf.history

# Accessing metrics directly from history
val_accuracy = history['val_accuracy']
val_logloss = history['val_logloss']
lr = history['lr']

# Plot Validation Accuracy
plt.figure(figsize=(10, 6))
plt.plot(val_accuracy, label='Validation Accuracy')
plt.title("Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# Plot Validation Loss
plt.figure(figsize=(10, 6))
plt.plot(val_logloss, label='Validation Loss')
plt.title("Validation Loss")
plt.xlabel('Epochs')
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Optionally, plot the learning rate evolution
plt.figure(figsize=(10, 6))
plt.plot(lr, label='Learning Rate')
plt.title("Learning Rate Over Epochs")
plt.xlabel('Epochs')
plt.ylabel("Learning Rate")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Initialize lists to store metrics for each epoch
precision_train_list = []
recall_train_list = []
f1_train_list = []
precision_val_list = []
recall_val_list = []
f1_val_list = []
accuracy_train_list = []

# Compute metrics for both training and validation sets for each epoch
for epoch in range(len(tabnet_clf.history['val_logloss'])):  # Use validation loss as a reference
    # Get predictions from the model on the training set
    y_pred_train = tabnet_clf.predict(X_train)

    # Calculate training metrics
    precision_train = precision_score(y_traino, y_pred_train)
    recall_train = recall_score(y_traino, y_pred_train)
    f1_train = f1_score(y_traino, y_pred_train)
    accuracy_train = accuracy_score(y_traino, y_pred_train)

    # Append training metrics to lists
    precision_train_list.append(precision_train)
    recall_train_list.append(recall_train)
    f1_train_list.append(f1_train)
    accuracy_train_list.append(accuracy_train)

    # Get predictions from the model on the validation set
    y_pred_val = tabnet_clf.predict(X_test)

    # Calculate validation metrics
    precision_val = precision_score(y_testo, y_pred_val)
    recall_val = recall_score(y_testo, y_pred_val)
    f1_val = f1_score(y_testo, y_pred_val)

    # Append validation metrics to lists
    precision_val_list.append(precision_val)
    recall_val_list.append(recall_val)
    f1_val_list.append(f1_val)

# Plot Accuracy
plt.figure(figsize=(10, 6))
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), tabnet_clf.history['val_accuracy'], label='Validation Accuracy')
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), accuracy_train_list, label='Training Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Precision
plt.figure(figsize=(10, 6))
plt.plot(range(len(precision_train_list)), precision_train_list, label='Training Precision')
plt.plot(range(len(precision_val_list)), precision_val_list, label='Validation Precision')
plt.title('Precision over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)
plt.show()

# Plot Recall
plt.figure(figsize=(10, 6))
plt.plot(range(len(recall_train_list)), recall_train_list, label='Training Recall')
plt.plot(range(len(recall_val_list)), recall_val_list, label='Validation Recall')
plt.title('Recall over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()
plt.grid(True)
plt.show()

# Plot F1 Score
plt.figure(figsize=(10, 6))
plt.plot(range(len(f1_train_list)), f1_train_list, label='Training F1 Score')
plt.plot(range(len(f1_val_list)), f1_val_list, label='Validation F1 Score')
plt.title('F1 Score over Epochs')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True)
plt.show()

"""# **3.D TabNET for predicting Anti_inflammatory_Activity**"""

import numpy as np
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch

# Ensure that X_train and X_test are numpy arrays, and y_train and y_test are numpy arrays
'''X_train = X_train.values
X_test = X_test.values'''
y_traini = y_traini.values
y_testi = y_testi.values

# Define and initialize the TabNet model
tabnet_clf = TabNetClassifier()

# Train TabNet on your Anti_Bacterial_Activity data (or any other target label)
history_single = tabnet_clf.fit(
    X_train, y_traini,
    eval_set=[(X_test, y_testi)],
    eval_name=["val"],
    eval_metric=["accuracy", "logloss"],  # Removed unsupported metrics
    max_epochs=100,
    patience=10,     # Early stopping if no improvement
    batch_size=1024,
    virtual_batch_size=128
)

# Predict for train and test set
y_predi_train = tabnet_clf.predict(X_train)
y_predi_test = tabnet_clf.predict(X_test)

# Calculate performance metrics on training data
accuracy_train = accuracy_score(y_traini, y_predi_train)
precision_train = precision_score(y_traini, y_predi_train)
recall_train = recall_score(y_traini, y_predi_train)
f1_train = f1_score(y_traini, y_predi_train)

print(f"TabNet Model Accuracy on Training Data for Anti_inflammatory_Activity: {accuracy_train * 100:.2f}%")
print(f"TabNet Model Precision on Training Data for Anti_inflammatory_Activity: {precision_train * 100:.2f}%")
print(f"TabNet Model Recall on Training Data for Anti_inflammatory_Activity: {recall_train * 100:.2f}%")
print(f"TabNet Model F1 Score on Training Data for Anti_inflammatory_Activity: {f1_train * 100:.2f}%")

# Calculate performance metrics on test data
accuracy_test = accuracy_score(y_testi, y_predi_test)
precision_test = precision_score(y_testi, y_predi_test)
recall_test = recall_score(y_testi, y_predi_test)
f1_test = f1_score(y_testi, y_predi_test)

print(f"TabNet Model Accuracy on Test Data for Anti_inflammatory_Activity: {accuracy_test * 100:.2f}%")
print(f"TabNet Model Precision on Test Data for Anti_inflammatory_Activity: {precision_test * 100:.2f}%")
print(f"TabNet Model Recall on Test Data for Anti_inflammatory_Activity: {recall_test * 100:.2f}%")
print(f"TabNet Model F1 Score on Test Data for Anti_inflammatory_Activity: {f1_test * 100:.2f}%")

# Save the model if necessary
tabnet_clf.save_model("tabnet_model_anti_inflammatory")

# Optionally, load the model in future if required
# tabnet_clf.load_model("tabnet_model_anti_inflammatory.zip")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_traini, y_predi_train)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Anti_inflammatory_Activity (Single-Task TabNet) - Training Data", fontsize=16)
plt.show()

# Generate and plot confusion matrix
cm_b = confusion_matrix(y_testi, y_predi_test)
disp_b = ConfusionMatrixDisplay(confusion_matrix=cm_b)

fig, ax = plt.subplots(figsize=(6, 6))
disp_b.plot(ax=ax, colorbar=False)
plt.title("Confusion Matrix - Anti_inflammatory_Activity (Single-Task TabNet) - Test Data", fontsize=16)
plt.show()

from torchviz import make_dot
import torch

# Input shape
input_shape = X_train.shape[1]

# Generate a random input with the same shape as your training data
x = torch.randn(1, input_shape)

# Pass the input through the model's network (forward pass)
y = tabnet_clf.network(x)  # Access the network attribute directly for forward pass

# Create a visualization of the computational graph
# Since 'named_parameters' is not available, we'll pass the network's state_dict instead
make_dot(y, params=dict(tabnet_clf.network.state_dict())).render("tabnet_clf_antiinflam", format="png")

# Display the generated model architecture
from IPython.display import Image
Image('tabnet_clf_antiinflam.png')

import matplotlib.pyplot as plt

# Function to plot learning curves for a metric
def plot_learning_curves(history, val_metric_name, metric_label, title):
    val_metric = history[val_metric_name] if val_metric_name in history else None

    plt.figure(figsize=(10, 6))

    if val_metric is not None:
        plt.plot(val_metric, label=f'Validation {metric_label}')

    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel(metric_label)
    plt.legend()
    plt.grid(True)
    plt.show()

# Extract history
history = tabnet_clf.history

# Accessing metrics directly from history
val_accuracy = history['val_accuracy']
val_logloss = history['val_logloss']
lr = history['lr']

# Plot Validation Accuracy
plt.figure(figsize=(10, 6))
plt.plot(val_accuracy, label='Validation Accuracy')
plt.title("Validation Accuracy")
plt.xlabel('Epochs')
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# Plot Validation Loss
plt.figure(figsize=(10, 6))
plt.plot(val_logloss, label='Validation Loss')
plt.title("Validation Loss")
plt.xlabel('Epochs')
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Optionally, plot the learning rate evolution
plt.figure(figsize=(10, 6))
plt.plot(lr, label='Learning Rate')
plt.title("Learning Rate Over Epochs")
plt.xlabel('Epochs')
plt.ylabel("Learning Rate")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Initialize lists to store metrics for each epoch
precision_train_list = []
recall_train_list = []
f1_train_list = []
precision_val_list = []
recall_val_list = []
f1_val_list = []
accuracy_train_list = []

# Compute metrics for both training and validation sets for each epoch
for epoch in range(len(tabnet_clf.history['val_logloss'])):  # Use validation loss as a reference
    # Get predictions from the model on the training set
    y_pred_train = tabnet_clf.predict(X_train)

    # Calculate training metrics
    precision_train = precision_score(y_traini, y_pred_train)
    recall_train = recall_score(y_traini, y_pred_train)
    f1_train = f1_score(y_traini, y_pred_train)
    accuracy_train = accuracy_score(y_traini, y_pred_train)

    # Append training metrics to lists
    precision_train_list.append(precision_train)
    recall_train_list.append(recall_train)
    f1_train_list.append(f1_train)
    accuracy_train_list.append(accuracy_train)

    # Get predictions from the model on the validation set
    y_pred_val = tabnet_clf.predict(X_test)

    # Calculate validation metrics
    precision_val = precision_score(y_testi, y_pred_val)
    recall_val = recall_score(y_testi, y_pred_val)
    f1_val = f1_score(y_testi, y_pred_val)

    # Append validation metrics to lists
    precision_val_list.append(precision_val)
    recall_val_list.append(recall_val)
    f1_val_list.append(f1_val)

# Plot Accuracy
plt.figure(figsize=(10, 6))
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), tabnet_clf.history['val_accuracy'], label='Validation Accuracy')
plt.plot(range(len(tabnet_clf.history['val_accuracy'])), accuracy_train_list, label='Training Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot Precision
plt.figure(figsize=(10, 6))
plt.plot(range(len(precision_train_list)), precision_train_list, label='Training Precision')
plt.plot(range(len(precision_val_list)), precision_val_list, label='Validation Precision')
plt.title('Precision over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)
plt.show()

# Plot Recall
plt.figure(figsize=(10, 6))
plt.plot(range(len(recall_train_list)), recall_train_list, label='Training Recall')
plt.plot(range(len(recall_val_list)), recall_val_list, label='Validation Recall')
plt.title('Recall over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()
plt.grid(True)
plt.show()

# Plot F1 Score
plt.figure(figsize=(10, 6))
plt.plot(range(len(f1_train_list)), f1_train_list, label='Training F1 Score')
plt.plot(range(len(f1_val_list)), f1_val_list, label='Validation F1 Score')
plt.title('F1 Score over Epochs')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True)
plt.show()

"""# **4. MULTITASK TABNET**"""

import numpy as np
from pytorch_tabnet.multitask import TabNetMultiTaskClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch

# Ensure that X_train and X_test are numpy arrays, and the labels are also numpy arrays
X_train = X_train.values
X_test = X_test.values
y_trainb = y_trainb.values
y_trainf = y_trainf.values
y_traini = y_traini.values
y_traino = y_traino.values
y_testb = y_testb.values
y_testf = y_testf.values
y_testi = y_testi.values
y_testo = y_testo.values

# Stack all the labels into a single array with multiple columns (one column per task)
y_train_multitask = np.column_stack([y_trainb, y_trainf, y_traini, y_traino])
y_test_multitask = np.column_stack([y_testb, y_testf, y_testi, y_testo])

# Define and initialize the TabNet multitask model
tabnet_clf = TabNetMultiTaskClassifier()

# Train TabNet on your multitask data
history_multitask = tabnet_clf.fit(
    X_train,
    y_train_multitask,
    max_epochs=100,
    patience=10,
    batch_size=1024,
    virtual_batch_size=128,
    num_workers=0,
    eval_metric=["accuracy", "logloss"]
)

import numpy as np

# Predict for train and test set
y_pred_train = np.array(tabnet_clf.predict(X_train)).T  # Transpose to shape (n_samples, n_tasks)
y_pred_test = np.array(tabnet_clf.predict(X_test)).T  # Transpose to shape (n_samples, n_tasks)

# Check the shape of predictions
print("Shape of y_pred_train:", y_pred_train.shape)
print("Shape of y_pred_test:", y_pred_test.shape)

# Extract predictions for each task
if y_pred_train.ndim == 2 and y_pred_train.shape[1] == 4:
    y_predb_train = y_pred_train[:, 0]  # For y_trainb
    y_predf_train = y_pred_train[:, 1]  # For y_trainf
    y_predi_train = y_pred_train[:, 2]  # For y_traini
    y_predo_train = y_pred_train[:, 3]  # For y_traino

    y_predb_test = y_pred_test[:, 0]
    y_predf_test = y_pred_test[:, 1]
    y_predi_test = y_pred_test[:, 2]
    y_predo_test = y_pred_test[:, 3]
else:
    print("Unexpected shape for predictions. Check the model output.")

# Convert predictions to binary values (0 or 1) after ensuring they are floats
y_predb_train = (y_predb_train.astype(float) > 0.5).astype(int)
y_predb_test = (y_predb_test.astype(float) > 0.5).astype(int)

y_predf_train = (y_predf_train.astype(float) > 0.5).astype(int)
y_predf_test = (y_predf_test.astype(float) > 0.5).astype(int)

y_predi_train = (y_predi_train.astype(float) > 0.5).astype(int)
y_predi_test = (y_predi_test.astype(float) > 0.5).astype(int)

y_predo_train = (y_predo_train.astype(float) > 0.5).astype(int)
y_predo_test = (y_predo_test.astype(float) > 0.5).astype(int)

# Define a function to calculate and print metrics for each task
def print_metrics(y_true, y_pred, task_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"Multitask TabNet Model Accuracy on {task_name} Data: {accuracy * 100:.2f}%")
    print(f"Multitask TabNet Model Precision on {task_name} Data: {precision * 100:.2f}%")
    print(f"Multitask TabNet Model Recall on {task_name} Data: {recall * 100:.2f}%")
    print(f"Multitask TabNet Model F1 Score on {task_name} Data: {f1 * 100:.2f}%\n")

# Calculate and print metrics for training data and test data
print_metrics(y_trainb, y_predb_train, "Training Anti_Bacterial_Activity")
print_metrics(y_testb, y_predb_test, "Test Anti_Bacterial_Activity")

print("\n")

print_metrics(y_trainf, y_predf_train, "Training Anti_Fungal_Activity")
print_metrics(y_testf, y_predf_test, "Test Anti_Fungal_Activity")

print("\n")

print_metrics(y_traini, y_predi_train, "Training Anti_Inflammatory_Activity")
print_metrics(y_testi, y_predi_test, "Test Anti_Inflammatory_Activity")

print("\n")

print_metrics(y_traino, y_predo_train, "Training Anti_Oxidant_Activity")
print_metrics(y_testo, y_predo_test, "Test Anti_Oxidant_Activity")

tabnet_clf.save_model("tabnet_model_multitask")

# Optionally, load the model in future if required
# tabnet_clf.load_model("tabnet_model_multitask.zip")

# Generate and plot confusion matrices for each task
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)

    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(title, fontsize=16)
    plt.show()

# Plot confusion matrices for training data & test data
plot_confusion_matrix(y_trainb, y_predb_train, "Confusion Matrix - Training Anti_Bacterial_Activity")
print("\n")
plot_confusion_matrix(y_testb, y_predb_test, "Confusion Matrix - Test Anti_Bacterial_Activity")

print("\n")
print("\n")

plot_confusion_matrix(y_trainf, y_predf_train, "Confusion Matrix - Training Anti_Fungal_Activity")
print("\n")
plot_confusion_matrix(y_testf, y_predf_test, "Confusion Matrix - Test Anti_Fungal_Activity")

print("\n")
print("\n")

plot_confusion_matrix(y_traini, y_predi_train, "Confusion Matrix - Training Anti_Inflammatory_Activity")
print("\n")
plot_confusion_matrix(y_testi, y_predi_test, "Confusion Matrix - Test Anti_Inflammatory_Activity")

print("\n")
print("\n")

plot_confusion_matrix(y_traino, y_predo_train, "Confusion Matrix - Training Anti_Oxidant_Activity")
print("\n")
plot_confusion_matrix(y_testo, y_predo_test, "Confusion Matrix - Test Anti_Oxidant_Activity")

!pip install torchviz

from torchviz import make_dot
import torch

# Input shape
input_shape = X_train.shape[1]

# Generate a random input with the same shape as your training data
x = torch.randn(1, input_shape)

# Pass the input through the model's network (forward pass)
output = tabnet_clf.network(x)

# Check if output is a tuple and contains tensors
if isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], list):
    # Extract the first tensor from the first element of the output tuple (which is a list of tensors)
    output_tensor = output[0][0]  # Extract the first tensor

    # Ensure output is a tensor for visualization
    if isinstance(output_tensor, torch.Tensor):
        # Visualize the model architecture
        dot = make_dot(output_tensor, params=dict(tabnet_clf.network.state_dict()))
        dot.render("tabnet_clf_multitask", format="png")

        # Display the generated image in notebook (optional)
        from IPython.display import Image
        display(Image('tabnet_clf_multitask.png'))
    else:
        print("The extracted output is still not a tensor.")
else:
    print("The output is not a tuple or does not contain the expected structure.")

"""# **5. Gradient Boosted Neural Network**

# **5.A For Antifungal**
"""

import torch
import torch.nn as nn
import torch.optim as optim

from sklearn.metrics import log_loss

X_train = torch.FloatTensor(X_train.values)
y_trainf = torch.FloatTensor(y_trainf.values).unsqueeze(1)  # Ensure target has the correct shape for BCELoss
X_test = torch.FloatTensor(X_test.values)
y_testf = torch.FloatTensor(y_testf.values).unsqueeze(1)

y_trainb = torch.FloatTensor(y_trainb.values).unsqueeze(1)
y_testb = torch.FloatTensor(y_testb.values).unsqueeze(1)

y_traino = torch.FloatTensor(y_traino.values).unsqueeze(1)
y_testo = torch.FloatTensor(y_testo.values).unsqueeze(1)

y_traini = torch.FloatTensor(y_traini.values).unsqueeze(1)
y_testi = torch.FloatTensor(y_testi.values).unsqueeze(1)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss

# Define the NeuralNet class with Dropout layer added
class NeuralNet(nn.Module):
    def __init__(self, input_dim, dropout_rate=0.5):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)  # Add dropout with the given rate
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)  # Output layer for binary classification

    def forward(self, x):
        # Pass input through the layers with dropout
        x = self.relu(self.fc1(x))
        x = self.dropout(x)  # Apply dropout after first layer
        x = self.relu(self.fc2(x))
        x = self.fc3(x)  # No sigmoid here, raw logits returned
        return x

# Define the GradBoostNet class
class GradBoostNet:
    def __init__(self, n_estimators=10, learning_rate=0.1, epochs=10, lr=0.001, dropout_rate=0.5):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.lr = lr
        self.dropout_rate = dropout_rate  # Include dropout rate
        self.models = []

    def fit(self, X_train, y_train, X_test, y_test):
        input_dim = X_train.shape[1]

        # Ensure target tensors are of correct shape and type
        y_train = y_train.view(-1, 1).float()
        y_test = y_test.view(-1, 1).float()

        # Initialize current predictions to zeros
        current_predictions = torch.zeros_like(y_train)

        metrics = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_f1': [], 'val_f1': []}

        for i in range(self.n_estimators):
            model = NeuralNet(input_dim, dropout_rate=self.dropout_rate)  # Pass dropout rate to NeuralNet
            optimizer = optim.Adam(model.parameters(), lr=self.lr)
            criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss

            # Calculate residuals based on current predictions
            residuals = y_train - current_predictions

            # Training loop
            for epoch in range(self.epochs):
                model.train()

                # Forward pass
                outputs = model(X_train)
                loss = criterion(outputs, residuals)  # Using residuals as targets

                # Backward pass and optimization
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # Store training loss
                metrics['train_loss'].append(loss.item())

                # Calculate training metrics
                y_pred_train = (torch.sigmoid(outputs).detach().numpy() > 0.5).astype(int)  # Apply sigmoid to outputs
                y_true_train = y_train.numpy().astype(int).flatten()

                train_acc = accuracy_score(y_true_train, y_pred_train)
                train_precision = precision_score(y_true_train, y_pred_train)
                train_recall = recall_score(y_true_train, y_pred_train)
                train_f1 = f1_score(y_true_train, y_pred_train)

                metrics['train_acc'].append(train_acc)
                metrics['train_precision'].append(train_precision)
                metrics['train_recall'].append(train_recall)
                metrics['train_f1'].append(train_f1)

                # Evaluate on validation set
                model.eval()
                with torch.no_grad():
                    val_outputs = model(X_test)
                    val_loss = criterion(val_outputs, y_test)
                    metrics['val_loss'].append(val_loss.item())

                    y_pred_test = (torch.sigmoid(val_outputs).detach().numpy() > 0.5).astype(int)
                    y_true_test = y_test.numpy().astype(int).flatten()

                    val_acc = accuracy_score(y_true_test, y_pred_test)
                    val_precision = precision_score(y_true_test, y_pred_test)
                    val_recall = recall_score(y_true_test, y_pred_test)
                    val_f1 = f1_score(y_true_test, y_pred_test)

                    metrics['val_acc'].append(val_acc)
                    metrics['val_precision'].append(val_precision)
                    metrics['val_recall'].append(val_recall)
                    metrics['val_f1'].append(val_f1)

                print(f'Epoch {epoch+1}/{self.epochs}, Train Loss: {loss.item()}, Val Loss: {val_loss.item()}')

            self.models.append(model)

            # Update current predictions using raw logits (sigmoid will be applied outside)
            current_predictions += self.learning_rate * torch.sigmoid(outputs.detach())

            train_loss = log_loss(y_train.cpu(), y_pred_train)  # Ensure you're comparing tensors on the CPU
            print(f'Estimator {i+1}, Log Loss: {train_loss}')

        return metrics

    def predict(self, X):
        y_pred = np.zeros(X.shape[0])

        for model in self.models:
            y_pred += self.learning_rate * model(torch.FloatTensor(X)).detach().numpy().squeeze()

        # Apply sigmoid to combine results for binary classification
        return (y_pred > 0.5).astype(int)

"""# **5.A For Predicting Antifungal**"""

# Fit GBNN to the training data
gbnn = GradBoostNet(n_estimators=5, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = gbnn.fit(X_train, y_trainf, X_test, y_testf)

# Predict on the test set
y_pred_test = gbnn.predict(X_test)
y_pred_train = gbnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using GBNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_trainf.numpy().astype(int), y_pred_train, dataset="Train")
print_metrics(y_testf.numpy().astype(int), y_pred_test)

"""# **5.B For Anti_Bacterial**"""

# Fit GradBoostNet to the training data
gbnn = GradBoostNet(n_estimators=5, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = gbnn.fit(X_train, y_trainb, X_test, y_testb)

# Predict on the test set
y_pred_test = gbnn.predict(X_test)
y_pred_train = gbnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using GBNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_trainb.numpy().astype(int), y_pred_train, dataset="Train")
print_metrics(y_testb.numpy().astype(int), y_pred_test)

"""# **5.C For Anti_oxidant**"""

# Fit GradBoostNet to the training data
gbnn = GradBoostNet(n_estimators=5, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = gbnn.fit(X_train, y_traino, X_test, y_testo)

# Predict on the test set
y_pred_test = gbnn.predict(X_test)
y_pred_train = gbnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using GBNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_traino.numpy().astype(int), y_pred_train, dataset="Train")
print_metrics(y_testo.numpy().astype(int), y_pred_test)

"""# **5.D For Anti_inflammatory**"""

# Fit GBNN to the training data
gbnn = GradBoostNet(n_estimators=5, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = gbnn.fit(X_train, y_traini, X_test, y_testi)

# Predict on the test set
y_pred_test = gbnn.predict(X_test)
y_pred_train = gbnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using GBNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_traini.numpy().astype(int), y_pred_train, dataset="Train")
print_metrics(y_testi.numpy().astype(int), y_pred_test)

"""# **6. Bagging Neural Networks**"""

X_train = X_train.values
X_test = X_test.values
y_trainf = y_trainf.values
y_testf = y_testf.values

y_trainb = y_trainb.values
y_testb = y_testb.values

y_traino = y_traino.values
y_testo = y_testo.values

y_traini = y_traini.values
y_testi = y_testi.values

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.utils import resample

class NeuralNet(nn.Module):
    def __init__(self, input_dim, dropout_rate=0.5):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(dropout_rate)

        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)
        self.dropout3 = nn.Dropout(dropout_rate)

        self.fc4 = nn.Linear(64, 32)
        self.bn4 = nn.BatchNorm1d(32)
        self.dropout4 = nn.Dropout(dropout_rate)

        self.fc5 = nn.Linear(32, 1)  # Output a single logit for binary classification

        self.relu = nn.ReLU()  # Define ReLU once to reuse

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout1(x)

        x = self.fc2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.dropout2(x)

        x = self.fc3(x)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.dropout3(x)

        x = self.fc4(x)
        x = self.bn4(x)
        x = self.relu(x)
        x = self.dropout4(x)

        x = self.fc5(x)  # Return logits (without sigmoid)
        return x

class BaggNet:
    def __init__(self, n_estimators=10, learning_rate=0.1, epochs=10, lr=0.001, dropout_rate=0.5):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.lr = lr
        self.dropout_rate = dropout_rate
        self.models = []

    def fit(self, X_train, y_train):
        input_dim = X_train.shape[1]
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        metrics = {'train_loss': [], 'train_acc': [], 'train_precision': [], 'train_recall': [], 'train_f1': []}

        for i in range(self.n_estimators):
            # Resample data
            X_resampled, y_resampled = resample(X_train, y_train, random_state=i)
            X_resampled = torch.FloatTensor(X_resampled).to(device)
            y_resampled = torch.FloatTensor(y_resampled).view(-1, 1).to(device)

            # Create and initialize model
            model = NeuralNet(input_dim, dropout_rate=self.dropout_rate).to(device)
            optimizer = optim.Adam(model.parameters(), lr=self.lr)
            criterion = nn.BCEWithLogitsLoss()

            for epoch in range(self.epochs):
                model.train()

                # Forward pass
                outputs = model(X_resampled)
                loss = criterion(outputs, y_resampled)

                # Backward pass and optimization
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # Store loss
                metrics['train_loss'].append(loss.item())

                # Predictions and evaluation (after applying sigmoid to outputs)
                y_pred_train = (torch.sigmoid(outputs).detach().cpu().numpy() > 0.5).astype(int)
                y_true_train = y_resampled.cpu().numpy().astype(int).flatten()

                # Compute and store metrics
                train_acc = accuracy_score(y_true_train, y_pred_train)
                train_precision = precision_score(y_true_train, y_pred_train, zero_division=1)
                train_recall = recall_score(y_true_train, y_pred_train, zero_division=1)
                train_f1 = f1_score(y_true_train, y_pred_train, zero_division=1)

                metrics['train_acc'].append(train_acc)
                metrics['train_precision'].append(train_precision)
                metrics['train_recall'].append(train_recall)
                metrics['train_f1'].append(train_f1)

            # Append trained model
            self.models.append(model)
            print(f'Estimator {i+1}, Train Loss: {metrics["train_loss"][-1]}')

        return metrics

    def predict(self, X):
        # Initialize array for predictions from each model
        predictions = np.zeros((len(self.models), X.shape[0]))

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        with torch.no_grad():
            for i, model in enumerate(self.models):
                # Get predictions from each model
                model_preds = torch.sigmoid(model(torch.FloatTensor(X).to(device))).detach().cpu().numpy().squeeze()
                predictions[i, :] = (model_preds > 0.5).astype(int)

        # Apply majority voting
        final_predictions = np.mean(predictions, axis=0) > 0.5

        return final_predictions.astype(int)

# Fit GBNN to the training data
bnn = BaggNet(n_estimators=10, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = bnn.fit(X_train, y_trainf)

# Predict on the test set
y_pred_test = bnn.predict(X_test)
y_pred_train = bnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using BNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_trainf.astype(int), y_pred_train, dataset="Train")
print_metrics(y_testf.astype(int), y_pred_test)

# Fit GBNN to the training data
bnn = BaggNet(n_estimators=10, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = bnn.fit(X_train, y_trainb)

# Predict on the test set
y_pred_test = bnn.predict(X_test)
y_pred_train = bnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using BNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_trainb.astype(int), y_pred_train, dataset="Train")
print_metrics(y_testb.astype(int), y_pred_test)

# Fit GBNN to the training data
bnn = BaggNet(n_estimators=10, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = bnn.fit(X_train, y_traino)

# Predict on the test set
y_pred_test = bnn.predict(X_test)
y_pred_train = bnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using BNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_traino.astype(int), y_pred_train, dataset="Train")
print_metrics(y_testo.astype(int), y_pred_test)

# Fit GBNN to the training data
bnn = BaggNet(n_estimators=10, learning_rate=0.1, epochs=3000, lr=0.001)
metrics = bnn.fit(X_train, y_traini)

# Predict on the test set
y_pred_test = bnn.predict(X_test)
y_pred_train = bnn.predict(X_train)

# Calculate accuracy, precision, recall, F1 score, and confusion matrix
def print_metrics(y_true, y_pred, dataset="Test"):
    acc = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f'{dataset} Metrics:')
    print(f'Accuracy: {acc:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
    fig, ax = plt.subplots(figsize=(6,6))
    disp.plot(ax=ax, colorbar=False)
    plt.title(f'Confusion Matrix for Anti_Fungal_Activity using BNN on {dataset} Data', fontsize=24)
    plt.xlabel("Predicted Value", fontsize=14)
    plt.ylabel("Actual Values", fontsize=14)
    plt.xticks(rotation=90)
    plt.show()
    print()

# Print metrics for both training and testing sets
print_metrics(y_traini.astype(int), y_pred_train, dataset="Train")
print_metrics(y_testi.astype(int), y_pred_test)

"""# **7. LSTM**

# **7.A For Antifungal**
"""

X_train = X_train.values
X_test = X_test.values
y_trainf = y_trainf.values
y_testf = y_testf.values

y_trainb = y_trainb.values
y_testb = y_testb.values

y_traino = y_traino.values
y_testo = y_testo.values

y_traini = y_traini.values
y_testi = y_testi.values

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit LSTM requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build LSTM model
model = Sequential()

model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second LSTM layer
model.add(LSTM(50, return_sequences=True))  # Return full sequence again to feed to the next LSTM layer
model.add(Dropout(0.2))

# Third LSTM layer (last one, no return_sequences here)
model.add(LSTM(50))  # No return_sequences as this is the final LSTM layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_trainf,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testf),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_trainf, y_train_pred_class)}")
print(f"Precision: {precision_score(y_trainf, y_train_pred_class)}")
print(f"Recall: {recall_score(y_trainf, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_trainf, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_trainf, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testf, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testf, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testf, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testf, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testf, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_trainf, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testf, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

"""# **7.B For AntiBacterial**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit LSTM requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build LSTM model
model = Sequential()

model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second LSTM layer
model.add(LSTM(50, return_sequences=True))  # Return full sequence again to feed to the next LSTM layer
model.add(Dropout(0.2))

# Third LSTM layer (last one, no return_sequences here)
model.add(LSTM(50))  # No return_sequences as this is the final LSTM layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model2.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_trainb,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testb),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model2.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_trainb, y_train_pred_class)}")
print(f"Precision: {precision_score(y_trainb, y_train_pred_class)}")
print(f"Recall: {recall_score(y_trainb, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_trainb, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_trainb, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testb, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testb, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testb, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testb, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testb, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_trainb, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testb, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



"""# **7.C For AntiOxidant**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit LSTM requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build LSTM model
model = Sequential()

model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second LSTM layer
model.add(LSTM(50, return_sequences=True))  # Return full sequence again to feed to the next LSTM layer
model.add(Dropout(0.2))

# Third LSTM layer (last one, no return_sequences here)
model.add(LSTM(50))  # No return_sequences as this is the final LSTM layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model4.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_traino,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testo),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model4.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_traino, y_train_pred_class)}")
print(f"Precision: {precision_score(y_traino, y_train_pred_class)}")
print(f"Recall: {recall_score(y_traino, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_traino, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_traino, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testo, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testo, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testo, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testo, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testo, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_traino, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testo, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

"""# **7.D For Anti_inflammatory**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit LSTM requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build LSTM model
model = Sequential()

model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second LSTM layer
model.add(LSTM(50, return_sequences=True))  # Return full sequence again to feed to the next LSTM layer
model.add(Dropout(0.2))

# Third LSTM layer (last one, no return_sequences here)
model.add(LSTM(50))  # No return_sequences as this is the final LSTM layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model5.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_traini,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testi),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model5.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_traini, y_train_pred_class)}")
print(f"Precision: {precision_score(y_traini, y_train_pred_class)}")
print(f"Recall: {recall_score(y_traini, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_traini, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_traini, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testi, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testi, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testi, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testi, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testi, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_traini, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testi, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

"""# **8. GRU**

# **8.A For Antifungal**
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit GRU requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build GRU model
model = Sequential()

model.add(GRU(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second GRU layer
model.add(GRU(50, return_sequences=True))  # Return full sequence again to feed to the next GRU layer
model.add(Dropout(0.2))

# Third GRU layer (last one, no return_sequences here)
model.add(GRU(50))  # No return_sequences as this is the final GRU layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_trainf,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testf),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_trainf, y_train_pred_class)}")
print(f"Precision: {precision_score(y_trainf, y_train_pred_class)}")
print(f"Recall: {recall_score(y_trainf, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_trainf, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_trainf, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testf, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testf, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testf, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testf, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testf, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_trainf, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testf, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



"""# **8.B For Antibacterial**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit GRU requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build GRU model
model = Sequential()

model.add(GRU(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second GRU layer
model.add(GRU(50, return_sequences=True))  # Return full sequence again to feed to the next GRU layer
model.add(Dropout(0.2))

# Third GRU layer (last one, no return_sequences here)
model.add(GRU(50))  # No return_sequences as this is the final GRU layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model6.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_trainb,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testb),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model6.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_trainb, y_train_pred_class)}")
print(f"Precision: {precision_score(y_trainb, y_train_pred_class)}")
print(f"Recall: {recall_score(y_trainb, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_trainb, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_trainb, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testb, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testb, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testb, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testb, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testb, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_trainb, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Bacterial_Activity using GRU on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testb, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Bacterial_Activity using GRU on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



"""# **8.C For Antioxidant**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit GRU requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build GRU model
model = Sequential()

model.add(GRU(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second GRU layer
model.add(GRU(50, return_sequences=True))  # Return full sequence again to feed to the next GRU layer
model.add(Dropout(0.2))

# Third GRU layer (last one, no return_sequences here)
model.add(GRU(50))  # No return_sequences as this is the final GRU layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model7.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_traino,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testo),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model7.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_traino, y_train_pred_class)}")
print(f"Precision: {precision_score(y_traino, y_train_pred_class)}")
print(f"Recall: {recall_score(y_traino, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_traino, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_traino, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testo, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testo, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testo, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testo, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testo, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_traino, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Antioxidant_Activity using GRU on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testo, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Antioxidant_Activity using GRU on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



"""# **8.D For Anti_inflammatory**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit GRU requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build GRU model
model = Sequential()

model.add(GRU(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second GRU layer
model.add(GRU(50, return_sequences=True))  # Return full sequence again to feed to the next GRU layer
model.add(Dropout(0.2))

# Third GRU layer (last one, no return_sequences here)
model.add(GRU(50))  # No return_sequences as this is the final GRU layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model8.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_traini,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testi),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model8.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_traini, y_train_pred_class)}")
print(f"Precision: {precision_score(y_traini, y_train_pred_class)}")
print(f"Recall: {recall_score(y_traini, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_traini, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_traini, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testi, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testi, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testi, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testi, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testi, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_traini, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Inflammatory_Activity using GRU on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testi, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Inflammatory_Activity using GRU on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



"""# **9. RNN**

# **9.A For Antifungal**
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit SimpleRNN requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build SimpleRNN model
model = Sequential()

model.add(SimpleRNN(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second SimpleRNN layer
model.add(SimpleRNN(50, return_sequences=True))  # Return full sequence again to feed to the next SimpleRNN layer
model.add(Dropout(0.2))

# Third SimpleRNN layer (last one, no return_sequences here)
model.add(SimpleRNN(50))  # No return_sequences as this is the final SimpleRNN layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1)

# Train the model
history = model.fit(X_train_reshaped,
                    y_trainf,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testf),
                    callbacks=[checkpoint_callback], batch_size = 32)

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_trainf, y_train_pred_class)}")
print(f"Precision: {precision_score(y_trainf, y_train_pred_class)}")
print(f"Recall: {recall_score(y_trainf, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_trainf, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_trainf, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testf, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testf, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testf, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testf, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testf, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_trainf, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testf, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Fungal_Activity using LSTM on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



"""# **9.B For Antibacterial**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit SimpleRNN requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build GRU model
model = Sequential()

model.add(SimpleRNN(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second SimpleRNN layer
model.add(SimpleRNN(50, return_sequences=True))  # Return full sequence again to feed to the next SimpleRNN layer
model.add(Dropout(0.2))

# Third SimpleRNN layer (last one, no return_sequences here)
model.add(SimpleRNN(50))  # No return_sequences as this is the final SimpleRNN layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model6.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1, batch_size=32)

# Train the model
history = model.fit(X_train_reshaped,
                    y_trainb,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testb),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model6.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_trainb, y_train_pred_class)}")
print(f"Precision: {precision_score(y_trainb, y_train_pred_class)}")
print(f"Recall: {recall_score(y_trainb, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_trainb, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_trainb, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testb, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testb, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testb, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testb, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testb, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_trainb, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Bacterial_Activity using GRU on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testb, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Bacterial_Activity using GRU on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

"""# **9.C For Antioxidant**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit SimpleRNN requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build SimpleRNN model
model = Sequential()

model.add(SimpleRNN(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second SimpleRNN layer
model.add(SimpleRNN(50, return_sequences=True))  # Return full sequence again to feed to the next SimpleRNN layer
model.add(Dropout(0.2))

# Third SimpleRNN layer (last one, no return_sequences here)
model.add(SimpleRNN(50))  # No return_sequences as this is the final SimpleRNN layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model7.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1, batch_size=32)

# Train the model
history = model.fit(X_train_reshaped,
                    y_traino,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testo),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model7.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_traino, y_train_pred_class)}")
print(f"Precision: {precision_score(y_traino, y_train_pred_class)}")
print(f"Recall: {recall_score(y_traino, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_traino, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_traino, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testo, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testo, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testo, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testo, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testo, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_traino, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Antioxidant_Activity using RNN on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()



disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testo, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Antioxidant_Activity using RNN on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

"""# **9.D For Anti_inflammatory**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

# Reshape the input to fit SimpleRNN requirements (samples, time steps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # shape becomes (3500, 6, 1)
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # shape becomes (1500, 6, 1)

# Build SimpleRNN model
model = Sequential()

model.add(SimpleRNN(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
model.add(Dropout(0.2))  # Optional dropout for regularization

# Second SimpleRNN layer
model.add(SimpleRNN(50, return_sequences=True))  # Return full sequence again to feed to the next SimpleRNN layer
model.add(Dropout(0.2))

# Third SimpleRNN layer (last one, no return_sequences here)
model.add(SimpleRNN(50))  # No return_sequences as this is the final SimpleRNN layer
model.add(Dropout(0.2))

# Fully connected (Dense) layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the ModelCheckpoint callback
# Save the model at the epoch where 'val_accuracy' is the highest
checkpoint_callback = ModelCheckpoint('best_model8.h5',
                                      monitor='val_accuracy',
                                      save_best_only=True,
                                      mode='max',
                                      verbose=1, batch_size=32)

# Train the model
history = model.fit(X_train_reshaped,
                    y_traini,
                    epochs=50,
                    validation_data=(X_test_reshaped, y_testi),
                    callbacks=[checkpoint_callback])

# Load the model from the best epoch based on validation accuracy
model.load_weights('best_model8.h5')

# Now you can evaluate the model or make predictions using the best model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get model predictions (for binary classification)
y_train_pred = model.predict(X_train_reshaped)
y_test_pred = model.predict(X_test_reshaped)

# For binary classification, round the predictions to 0 or 1
y_train_pred_class = (y_train_pred > 0.5).astype(int)
y_test_pred_class = (y_test_pred > 0.5).astype(int)

# Training data metrics
print("Training Metrics:")
print(f"Accuracy: {accuracy_score(y_traini, y_train_pred_class)}")
print(f"Precision: {precision_score(y_traini, y_train_pred_class)}")
print(f"Recall: {recall_score(y_traini, y_train_pred_class)}")
print(f"F1 Score: {f1_score(y_traini, y_train_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_traini, y_train_pred_class)}")

# Test data metrics
print("\nTest Metrics:")
print(f"Accuracy: {accuracy_score(y_testi, y_test_pred_class)}")
print(f"Precision: {precision_score(y_testi, y_test_pred_class)}")
print(f"Recall: {recall_score(y_testi, y_test_pred_class)}")
print(f"F1 Score: {f1_score(y_testi, y_test_pred_class)}")
print(f"Confusion Matrix:\n{confusion_matrix(y_testi, y_test_pred_class)}")

import matplotlib.pyplot as plt
# Plotting accuracy and loss
# Extract accuracy and loss for both training and validation
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_accuracy = history.history.get('accuracy', history.history.get('acc'))  # 'accuracy' for modern versions
val_accuracy = history.history.get('val_accuracy', history.history.get('val_acc'))

# Create subplots: one for loss, one for accuracy
fig, axs = plt.subplots(2, figsize=(10, 8))

# Plot training & validation loss
axs[0].plot(train_loss, label='Train Loss')
axs[0].plot(val_loss, label='Validation Loss')
axs[0].set_title('Loss Curve')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend()

# Plot training & validation accuracy
axs[1].plot(train_accuracy, label='Train Accuracy')
axs[1].plot(val_accuracy, label='Validation Accuracy')
axs[1].set_title('Accuracy Curve')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Show the plot
plt.tight_layout()
plt.show()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_traini, y_train_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Inflammatory_Activity using RNN on Training Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()

disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_testi, y_test_pred_class))
fig, ax = plt.subplots(figsize=(6,6))
disp.plot(ax=ax, colorbar=False)
plt.title(f'Confusion Matrix for Anti_Inflammatory_Activity using RNN on Testing Data', fontsize=24)
plt.xlabel("Predicted Value", fontsize=14)
plt.ylabel("Actual Values", fontsize=14)
plt.xticks(rotation=90)
plt.show()
print()